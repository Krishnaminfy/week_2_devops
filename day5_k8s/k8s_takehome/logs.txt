
==> Audit <==
|--------------|-------------------------------|----------|-----------------------|---------|---------------------|---------------------|
|   Command    |             Args              | Profile  |         User          | Version |     Start Time      |      End Time       |
|--------------|-------------------------------|----------|-----------------------|---------|---------------------|---------------------|
| start        |                               | minikube | ADITYAREDDYDARA\Minfy | v1.36.0 | 17 Jun 25 13:59 IST |                     |
| update-check |                               | minikube | ADITYAREDDYDARA\Minfy | v1.36.0 | 17 Jun 25 14:58 IST | 17 Jun 25 14:58 IST |
| update-check |                               | minikube | ADITYAREDDYDARA\Minfy | v1.36.0 | 17 Jun 25 14:59 IST | 17 Jun 25 14:59 IST |
| start        | --driver=docker               | minikube | ADITYAREDDYDARA\Minfy | v1.36.0 | 17 Jun 25 16:24 IST |                     |
| start        |                               | minikube | ADITYAREDDYDARA\Minfy | v1.36.0 | 17 Jun 25 16:28 IST |                     |
| delete       |                               | minikube | ADITYAREDDYDARA\Minfy | v1.36.0 | 17 Jun 25 16:31 IST | 17 Jun 25 16:31 IST |
| start        |                               | minikube | ADITYAREDDYDARA\Minfy | v1.36.0 | 17 Jun 25 16:33 IST | 17 Jun 25 16:34 IST |
| stop         |                               | minikube | ADITYAREDDYDARA\Minfy | v1.36.0 | 17 Jun 25 16:40 IST | 17 Jun 25 16:40 IST |
| start        | --driver=docker               | minikube | ADITYAREDDYDARA\Minfy | v1.36.0 | 18 Jun 25 14:30 IST |                     |
| start        | --driver=docker               | minikube | ADITYAREDDYDARA\Minfy | v1.36.0 | 18 Jun 25 16:08 IST | 18 Jun 25 16:08 IST |
| stop         |                               | minikube | ADITYAREDDYDARA\Minfy | v1.36.0 | 18 Jun 25 16:13 IST | 18 Jun 25 16:13 IST |
| start        | --driver=docker               | minikube | ADITYAREDDYDARA\Minfy | v1.36.0 | 18 Jun 25 16:42 IST | 18 Jun 25 16:43 IST |
| service      | mongo-express-service         | minikube | ADITYAREDDYDARA\Minfy | v1.36.0 | 18 Jun 25 16:53 IST |                     |
| service      | krishna-mongo-express-service | minikube | ADITYAREDDYDARA\Minfy | v1.36.0 | 18 Jun 25 16:53 IST |                     |
| service      | mongo-express-service         | minikube | ADITYAREDDYDARA\Minfy | v1.36.0 | 18 Jun 25 16:53 IST |                     |
| service      | mongo-express-service         | minikube | ADITYAREDDYDARA\Minfy | v1.36.0 | 18 Jun 25 16:59 IST |                     |
| service      | mongo-express-service         | minikube | ADITYAREDDYDARA\Minfy | v1.36.0 | 18 Jun 25 17:00 IST |                     |
| service      | mongo-express-service         | minikube | ADITYAREDDYDARA\Minfy | v1.36.0 | 18 Jun 25 17:02 IST |                     |
|--------------|-------------------------------|----------|-----------------------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2025/06/18 16:42:29
Running on machine: AdityaReddyDaram
Binary: Built with gc go1.24.0 for windows/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0618 16:42:29.481865    7508 out.go:345] Setting OutFile to fd 100 ...
I0618 16:42:29.482805    7508 out.go:392] TERM=,COLORTERM=, which probably does not support color
I0618 16:42:29.482805    7508 out.go:358] Setting ErrFile to fd 104...
I0618 16:42:29.482805    7508 out.go:392] TERM=,COLORTERM=, which probably does not support color
I0618 16:42:29.500824    7508 out.go:352] Setting JSON to false
I0618 16:42:29.506826    7508 start.go:130] hostinfo: {"hostname":"AdityaReddyDaram","uptime":88525,"bootTime":1750156624,"procs":278,"os":"windows","platform":"Microsoft Windows 10 Enterprise","platformFamily":"Standalone Workstation","platformVersion":"10.0.19045.5965 Build 19045.5965","kernelVersion":"10.0.19045.5965 Build 19045.5965","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"","hostId":"69600ee6-65ec-4e26-a1ad-28d8b229f64f"}
W0618 16:42:29.507325    7508 start.go:138] gopshost.Virtualization returned error: not implemented yet
I0618 16:42:29.510324    7508 out.go:177] * minikube v1.36.0 on Microsoft Windows 10 Enterprise 10.0.19045.5965 Build 19045.5965
I0618 16:42:29.515322    7508 notify.go:220] Checking for updates...
I0618 16:42:29.515822    7508 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0618 16:42:29.517321    7508 driver.go:404] Setting default libvirt URI to qemu:///system
I0618 16:42:29.616770    7508 docker.go:123] docker version: linux-28.0.4:Docker Desktop 4.40.0 (187762)
I0618 16:42:29.620281    7508 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0618 16:42:30.351756    7508 info.go:266] docker info: {ID:afa79312-5004-4bd3-a74f-7c8e0acb49d4 Containers:43 ContainersRunning:34 ContainersPaused:0 ContainersStopped:9 Images:24 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:164 OomKillDisable:true NGoroutines:160 SystemTime:2025-06-18 11:12:30.26969633 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:17 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:4021051392 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.0.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:753481ec61c7c8955a23d6ff7bc8e4daed455734 Expected:753481ec61c7c8955a23d6ff7bc8e4daed455734} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:v1.2.5-0-g59923ef} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.3] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.22.0-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:0.2.20] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.34.0-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.6] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.0]] Warnings:<nil>}}
I0618 16:42:30.354272    7508 out.go:177] * Using the docker driver based on existing profile
I0618 16:42:30.359811    7508 start.go:304] selected driver: docker
I0618 16:42:30.359811    7508 start.go:908] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.67.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Minfy.ADITYAREDDYDARA:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0618 16:42:30.359811    7508 start.go:919] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0618 16:42:30.367105    7508 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0618 16:42:30.554512    7508 info.go:266] docker info: {ID:afa79312-5004-4bd3-a74f-7c8e0acb49d4 Containers:43 ContainersRunning:34 ContainersPaused:0 ContainersStopped:9 Images:24 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:false BridgeNfIP6Tables:false Debug:false NFd:164 OomKillDisable:true NGoroutines:160 SystemTime:2025-06-18 11:12:30.541624169 +0000 UTC LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:17 KernelVersion:5.15.167.4-microsoft-standard-WSL2 OperatingSystem:Docker Desktop OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[::1/128 127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:8 MemTotal:4021051392 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy:http.docker.internal:3128 HTTPSProxy:http.docker.internal:3128 NoProxy:hubproxy.docker.internal Name:docker-desktop Labels:[com.docker.desktop.address=npipe://\\.\pipe\docker_cli] ExperimentalBuild:false ServerVersion:28.0.4 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:753481ec61c7c8955a23d6ff7bc8e4daed455734 Expected:753481ec61c7c8955a23d6ff7bc8e4daed455734} RuncCommit:{ID:v1.2.5-0-g59923ef Expected:v1.2.5-0-g59923ef} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=seccomp,profile=unconfined] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support WARNING: DOCKER_INSECURE_NO_IPTABLES_RAW is set WARNING: daemon is not using the default seccomp profile] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:ai Path:C:\Program Files\Docker\cli-plugins\docker-ai.exe SchemaVersion:0.1.0 ShortDescription:Docker AI Agent - Ask Gordon Vendor:Docker Inc. Version:v1.1.3] map[Name:buildx Path:C:\Program Files\Docker\cli-plugins\docker-buildx.exe SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.22.0-desktop.1] map[Name:cloud Path:C:\Program Files\Docker\cli-plugins\docker-cloud.exe SchemaVersion:0.1.0 ShortDescription:Docker Cloud Vendor:Docker Inc. Version:0.2.20] map[Name:compose Path:C:\Program Files\Docker\cli-plugins\docker-compose.exe SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.34.0-desktop.1] map[Name:debug Path:C:\Program Files\Docker\cli-plugins\docker-debug.exe SchemaVersion:0.1.0 ShortDescription:Get a shell into any image or container Vendor:Docker Inc. Version:0.0.38] map[Name:desktop Path:C:\Program Files\Docker\cli-plugins\docker-desktop.exe SchemaVersion:0.1.0 ShortDescription:Docker Desktop commands (Beta) Vendor:Docker Inc. Version:v0.1.6] map[Name:dev Path:C:\Program Files\Docker\cli-plugins\docker-dev.exe SchemaVersion:0.1.0 ShortDescription:Docker Dev Environments Vendor:Docker Inc. Version:v0.1.2] map[Name:extension Path:C:\Program Files\Docker\cli-plugins\docker-extension.exe SchemaVersion:0.1.0 ShortDescription:Manages Docker extensions Vendor:Docker Inc. Version:v0.2.27] map[Name:init Path:C:\Program Files\Docker\cli-plugins\docker-init.exe SchemaVersion:0.1.0 ShortDescription:Creates Docker-related starter files for your project Vendor:Docker Inc. Version:v1.4.0] map[Name:sbom Path:C:\Program Files\Docker\cli-plugins\docker-sbom.exe SchemaVersion:0.1.0 ShortDescription:View the packaged-based Software Bill Of Materials (SBOM) for an image URL:https://github.com/docker/sbom-cli-plugin Vendor:Anchore Inc. Version:0.6.0] map[Name:scout Path:C:\Program Files\Docker\cli-plugins\docker-scout.exe SchemaVersion:0.1.0 ShortDescription:Docker Scout Vendor:Docker Inc. Version:v1.17.0]] Warnings:<nil>}}
I0618 16:42:30.667453    7508 cni.go:84] Creating CNI manager for ""
I0618 16:42:30.668185    7508 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0618 16:42:30.668223    7508 start.go:347] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.67.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Minfy.ADITYAREDDYDARA:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0618 16:42:30.672051    7508 out.go:177] * Starting "minikube" primary control-plane node in "minikube" cluster
I0618 16:42:30.678303    7508 cache.go:121] Beginning downloading kic base image for docker with docker
I0618 16:42:30.681058    7508 out.go:177] * Pulling base image v0.0.47 ...
I0618 16:42:30.690600    7508 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0618 16:42:30.690600    7508 image.go:81] Checking for gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local docker daemon
I0618 16:42:30.691362    7508 preload.go:146] Found local preload: C:\Users\Minfy.ADITYAREDDYDARA\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4
I0618 16:42:30.691480    7508 cache.go:56] Caching tarball of preloaded images
I0618 16:42:30.692404    7508 preload.go:172] Found C:\Users\Minfy.ADITYAREDDYDARA\.minikube\cache\preloaded-tarball\preloaded-images-k8s-v18-v1.33.1-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0618 16:42:30.692404    7508 cache.go:59] Finished verifying existence of preloaded tar for v1.33.1 on docker
I0618 16:42:30.693360    7508 profile.go:143] Saving config to C:\Users\Minfy.ADITYAREDDYDARA\.minikube\profiles\minikube\config.json ...
I0618 16:42:30.793063    7508 image.go:100] Found gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b in local docker daemon, skipping pull
I0618 16:42:30.793554    7508 cache.go:145] gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b exists in daemon, skipping load
I0618 16:42:30.793554    7508 cache.go:230] Successfully downloaded all kic artifacts
I0618 16:42:30.794055    7508 start.go:360] acquireMachinesLock for minikube: {Name:mk7000929ebca6f888dca14b480a24113bf0752f Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0618 16:42:30.794055    7508 start.go:364] duration metric: took 0s to acquireMachinesLock for "minikube"
I0618 16:42:30.794055    7508 start.go:96] Skipping create...Using existing machine configuration
I0618 16:42:30.794554    7508 fix.go:54] fixHost starting: 
I0618 16:42:30.800568    7508 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0618 16:42:30.892289    7508 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0618 16:42:30.892289    7508 fix.go:138] unexpected machine state, will restart: <nil>
I0618 16:42:30.895290    7508 out.go:177] * Restarting existing docker container for "minikube" ...
I0618 16:42:30.903287    7508 cli_runner.go:164] Run: docker start minikube
I0618 16:42:31.755120    7508 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0618 16:42:31.809451    7508 kic.go:430] container "minikube" state is running.
I0618 16:42:31.816414    7508 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0618 16:42:31.863571    7508 profile.go:143] Saving config to C:\Users\Minfy.ADITYAREDDYDARA\.minikube\profiles\minikube\config.json ...
I0618 16:42:31.865697    7508 machine.go:93] provisionDockerMachine start ...
I0618 16:42:31.870073    7508 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0618 16:42:31.923017    7508 main.go:141] libmachine: Using SSH client type: native
I0618 16:42:31.923518    7508 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x12aa9e0] 0x12ad520 <nil>  [] 0s} 127.0.0.1 61837 <nil> <nil>}
I0618 16:42:31.923518    7508 main.go:141] libmachine: About to run SSH command:
hostname
I0618 16:42:31.927517    7508 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: EOF
I0618 16:42:35.122020    7508 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0618 16:42:35.122903    7508 ubuntu.go:169] provisioning hostname "minikube"
I0618 16:42:35.126404    7508 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0618 16:42:35.173902    7508 main.go:141] libmachine: Using SSH client type: native
I0618 16:42:35.174415    7508 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x12aa9e0] 0x12ad520 <nil>  [] 0s} 127.0.0.1 61837 <nil> <nil>}
I0618 16:42:35.174415    7508 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0618 16:42:35.423386    7508 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0618 16:42:35.430385    7508 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0618 16:42:35.488192    7508 main.go:141] libmachine: Using SSH client type: native
I0618 16:42:35.488705    7508 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x12aa9e0] 0x12ad520 <nil>  [] 0s} 127.0.0.1 61837 <nil> <nil>}
I0618 16:42:35.488705    7508 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0618 16:42:35.632035    7508 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0618 16:42:35.632208    7508 ubuntu.go:175] set auth options {CertDir:C:\Users\Minfy.ADITYAREDDYDARA\.minikube CaCertPath:C:\Users\Minfy.ADITYAREDDYDARA\.minikube\certs\ca.pem CaPrivateKeyPath:C:\Users\Minfy.ADITYAREDDYDARA\.minikube\certs\ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:C:\Users\Minfy.ADITYAREDDYDARA\.minikube\machines\server.pem ServerKeyPath:C:\Users\Minfy.ADITYAREDDYDARA\.minikube\machines\server-key.pem ClientKeyPath:C:\Users\Minfy.ADITYAREDDYDARA\.minikube\certs\key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:C:\Users\Minfy.ADITYAREDDYDARA\.minikube\certs\cert.pem ServerCertSANs:[] StorePath:C:\Users\Minfy.ADITYAREDDYDARA\.minikube}
I0618 16:42:35.632208    7508 ubuntu.go:177] setting up certificates
I0618 16:42:35.632208    7508 provision.go:84] configureAuth start
I0618 16:42:35.635294    7508 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0618 16:42:35.686872    7508 provision.go:143] copyHostCerts
I0618 16:42:35.688335    7508 exec_runner.go:144] found C:\Users\Minfy.ADITYAREDDYDARA\.minikube/ca.pem, removing ...
I0618 16:42:35.688335    7508 exec_runner.go:203] rm: C:\Users\Minfy.ADITYAREDDYDARA\.minikube\ca.pem
I0618 16:42:35.688834    7508 exec_runner.go:151] cp: C:\Users\Minfy.ADITYAREDDYDARA\.minikube\certs\ca.pem --> C:\Users\Minfy.ADITYAREDDYDARA\.minikube/ca.pem (1074 bytes)
I0618 16:42:35.691837    7508 exec_runner.go:144] found C:\Users\Minfy.ADITYAREDDYDARA\.minikube/cert.pem, removing ...
I0618 16:42:35.691837    7508 exec_runner.go:203] rm: C:\Users\Minfy.ADITYAREDDYDARA\.minikube\cert.pem
I0618 16:42:35.692334    7508 exec_runner.go:151] cp: C:\Users\Minfy.ADITYAREDDYDARA\.minikube\certs\cert.pem --> C:\Users\Minfy.ADITYAREDDYDARA\.minikube/cert.pem (1119 bytes)
I0618 16:42:35.693442    7508 exec_runner.go:144] found C:\Users\Minfy.ADITYAREDDYDARA\.minikube/key.pem, removing ...
I0618 16:42:35.693442    7508 exec_runner.go:203] rm: C:\Users\Minfy.ADITYAREDDYDARA\.minikube\key.pem
I0618 16:42:35.693875    7508 exec_runner.go:151] cp: C:\Users\Minfy.ADITYAREDDYDARA\.minikube\certs\key.pem --> C:\Users\Minfy.ADITYAREDDYDARA\.minikube/key.pem (1675 bytes)
I0618 16:42:35.694834    7508 provision.go:117] generating server cert: C:\Users\Minfy.ADITYAREDDYDARA\.minikube\machines\server.pem ca-key=C:\Users\Minfy.ADITYAREDDYDARA\.minikube\certs\ca.pem private-key=C:\Users\Minfy.ADITYAREDDYDARA\.minikube\certs\ca-key.pem org=Minfy.minikube san=[127.0.0.1 192.168.67.2 localhost minikube]
I0618 16:42:35.985942    7508 provision.go:177] copyRemoteCerts
I0618 16:42:35.992759    7508 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0618 16:42:35.997262    7508 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0618 16:42:36.049020    7508 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61837 SSHKeyPath:C:\Users\Minfy.ADITYAREDDYDARA\.minikube\machines\minikube\id_rsa Username:docker}
I0618 16:42:36.165554    7508 ssh_runner.go:362] scp C:\Users\Minfy.ADITYAREDDYDARA\.minikube\certs\ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0618 16:42:36.205212    7508 ssh_runner.go:362] scp C:\Users\Minfy.ADITYAREDDYDARA\.minikube\machines\server.pem --> /etc/docker/server.pem (1176 bytes)
I0618 16:42:36.249860    7508 ssh_runner.go:362] scp C:\Users\Minfy.ADITYAREDDYDARA\.minikube\machines\server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0618 16:42:36.278614    7508 provision.go:87] duration metric: took 646.1452ms to configureAuth
I0618 16:42:36.278614    7508 ubuntu.go:193] setting minikube options for container-runtime
I0618 16:42:36.279115    7508 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0618 16:42:36.283116    7508 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0618 16:42:36.338652    7508 main.go:141] libmachine: Using SSH client type: native
I0618 16:42:36.339116    7508 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x12aa9e0] 0x12ad520 <nil>  [] 0s} 127.0.0.1 61837 <nil> <nil>}
I0618 16:42:36.339116    7508 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0618 16:42:36.507462    7508 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0618 16:42:36.507462    7508 ubuntu.go:71] root file system type: overlay
I0618 16:42:36.507944    7508 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0618 16:42:36.511443    7508 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0618 16:42:36.575549    7508 main.go:141] libmachine: Using SSH client type: native
I0618 16:42:36.577601    7508 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x12aa9e0] 0x12ad520 <nil>  [] 0s} 127.0.0.1 61837 <nil> <nil>}
I0618 16:42:36.577601    7508 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %s "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0618 16:42:36.758635    7508 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0618 16:42:36.766680    7508 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0618 16:42:36.828764    7508 main.go:141] libmachine: Using SSH client type: native
I0618 16:42:36.829283    7508 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x12aa9e0] 0x12ad520 <nil>  [] 0s} 127.0.0.1 61837 <nil> <nil>}
I0618 16:42:36.829283    7508 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0618 16:42:36.974611    7508 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0618 16:42:36.974611    7508 machine.go:96] duration metric: took 5.1089138s to provisionDockerMachine
I0618 16:42:36.975390    7508 start.go:293] postStartSetup for "minikube" (driver="docker")
I0618 16:42:36.975390    7508 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0618 16:42:36.987627    7508 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0618 16:42:36.991127    7508 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0618 16:42:37.041345    7508 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61837 SSHKeyPath:C:\Users\Minfy.ADITYAREDDYDARA\.minikube\machines\minikube\id_rsa Username:docker}
I0618 16:42:37.144829    7508 ssh_runner.go:195] Run: cat /etc/os-release
I0618 16:42:37.150442    7508 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0618 16:42:37.150442    7508 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0618 16:42:37.150442    7508 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0618 16:42:37.150442    7508 info.go:137] Remote host: Ubuntu 22.04.5 LTS
I0618 16:42:37.150943    7508 filesync.go:126] Scanning C:\Users\Minfy.ADITYAREDDYDARA\.minikube\addons for local assets ...
I0618 16:42:37.151456    7508 filesync.go:126] Scanning C:\Users\Minfy.ADITYAREDDYDARA\.minikube\files for local assets ...
I0618 16:42:37.151456    7508 start.go:296] duration metric: took 176.066ms for postStartSetup
I0618 16:42:37.161442    7508 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0618 16:42:37.165203    7508 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0618 16:42:37.221652    7508 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61837 SSHKeyPath:C:\Users\Minfy.ADITYAREDDYDARA\.minikube\machines\minikube\id_rsa Username:docker}
I0618 16:42:37.335140    7508 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0618 16:42:37.344876    7508 fix.go:56] duration metric: took 6.5503218s for fixHost
I0618 16:42:37.344876    7508 start.go:83] releasing machines lock for "minikube", held for 6.5508209s
I0618 16:42:37.350375    7508 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0618 16:42:37.421443    7508 ssh_runner.go:195] Run: curl.exe -sS -m 2 https://registry.k8s.io/
I0618 16:42:37.428442    7508 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0618 16:42:37.430443    7508 ssh_runner.go:195] Run: cat /version.json
I0618 16:42:37.434965    7508 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0618 16:42:37.491943    7508 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61837 SSHKeyPath:C:\Users\Minfy.ADITYAREDDYDARA\.minikube\machines\minikube\id_rsa Username:docker}
I0618 16:42:37.494942    7508 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61837 SSHKeyPath:C:\Users\Minfy.ADITYAREDDYDARA\.minikube\machines\minikube\id_rsa Username:docker}
W0618 16:42:37.603986    7508 start.go:867] [curl.exe -sS -m 2 https://registry.k8s.io/] failed: curl.exe -sS -m 2 https://registry.k8s.io/: Process exited with status 127
stdout:

stderr:
bash: line 1: curl.exe: command not found
I0618 16:42:37.627482    7508 ssh_runner.go:195] Run: systemctl --version
I0618 16:42:37.660691    7508 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0618 16:42:37.682032    7508 ssh_runner.go:195] Run: sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
W0618 16:42:37.708965    7508 start.go:439] unable to name loopback interface in configureRuntimes: unable to patch loopback cni config "/etc/cni/net.d/*loopback.conf*": sudo find \etc\cni\net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;: Process exited with status 1
stdout:

stderr:
find: '\\etc\\cni\\net.d': No such file or directory
I0618 16:42:37.717129    7508 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%p, " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0618 16:42:37.727129    7508 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0618 16:42:37.727629    7508 start.go:495] detecting cgroup driver to use...
I0618 16:42:37.727629    7508 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0618 16:42:37.729129    7508 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0618 16:42:37.752621    7508 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.10"|' /etc/containerd/config.toml"
I0618 16:42:37.770743    7508 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0618 16:42:37.781757    7508 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0618 16:42:37.788838    7508 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0618 16:42:37.810931    7508 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0618 16:42:37.835243    7508 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0618 16:42:37.859534    7508 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0618 16:42:37.877612    7508 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0618 16:42:37.933408    7508 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0618 16:42:37.961252    7508 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0618 16:42:37.986101    7508 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
W0618 16:42:38.002711    7508 out.go:270] ! Failing to connect to https://registry.k8s.io/ from inside the minikube container
W0618 16:42:38.005190    7508 out.go:270] * To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0618 16:42:38.008683    7508 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0618 16:42:38.028627    7508 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0618 16:42:38.047084    7508 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0618 16:42:38.195270    7508 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0618 16:42:38.387471    7508 start.go:495] detecting cgroup driver to use...
I0618 16:42:38.387471    7508 detect.go:187] detected "cgroupfs" cgroup driver on host os
I0618 16:42:38.399671    7508 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0618 16:42:38.419734    7508 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0618 16:42:38.428734    7508 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0618 16:42:38.445481    7508 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %s "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0618 16:42:38.468985    7508 ssh_runner.go:195] Run: which cri-dockerd
I0618 16:42:38.480485    7508 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0618 16:42:38.489004    7508 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (190 bytes)
I0618 16:42:38.528060    7508 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0618 16:42:38.651847    7508 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0618 16:42:38.758065    7508 docker.go:587] configuring docker to use "cgroupfs" as cgroup driver...
I0618 16:42:38.758065    7508 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0618 16:42:38.794861    7508 ssh_runner.go:195] Run: sudo systemctl reset-failed docker
I0618 16:42:38.814626    7508 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0618 16:42:38.916198    7508 ssh_runner.go:195] Run: sudo systemctl restart docker
I0618 16:42:44.178739    7508 ssh_runner.go:235] Completed: sudo systemctl restart docker: (5.2625416s)
I0618 16:42:44.185273    7508 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0618 16:42:44.206511    7508 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0618 16:42:44.228811    7508 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0618 16:42:44.246338    7508 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0618 16:42:44.322791    7508 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0618 16:42:44.436132    7508 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0618 16:42:44.555371    7508 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0618 16:42:44.590371    7508 ssh_runner.go:195] Run: sudo systemctl reset-failed cri-docker.service
I0618 16:42:44.615207    7508 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0618 16:42:44.760583    7508 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0618 16:42:45.420857    7508 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0618 16:42:45.431502    7508 start.go:542] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0618 16:42:45.438014    7508 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0618 16:42:45.447722    7508 start.go:563] Will wait 60s for crictl version
I0618 16:42:45.454267    7508 ssh_runner.go:195] Run: which crictl
I0618 16:42:45.470240    7508 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0618 16:42:45.833936    7508 start.go:579] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  28.1.1
RuntimeApiVersion:  v1
I0618 16:42:45.837915    7508 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0618 16:42:46.112487    7508 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0618 16:42:46.165522    7508 out.go:235] * Preparing Kubernetes v1.33.1 on Docker 28.1.1 ...
I0618 16:42:46.171516    7508 cli_runner.go:164] Run: docker exec -t minikube dig +short host.docker.internal
I0618 16:42:46.357665    7508 network.go:96] got host ip for mount in container by digging dns: 192.168.65.254
I0618 16:42:46.365250    7508 ssh_runner.go:195] Run: grep 192.168.65.254	host.minikube.internal$ /etc/hosts
I0618 16:42:46.371816    7508 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.65.254	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0618 16:42:46.399239    7508 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0618 16:42:46.472857    7508 kubeadm.go:875] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.67.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Minfy.ADITYAREDDYDARA:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0618 16:42:46.473358    7508 preload.go:131] Checking if preload exists for k8s version v1.33.1 and runtime docker
I0618 16:42:46.477857    7508 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0618 16:42:46.519593    7508 docker.go:702] Got preloaded images: -- stdout --
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5
nginx:1.14.2

-- /stdout --
I0618 16:42:46.519593    7508 docker.go:632] Images already preloaded, skipping extraction
I0618 16:42:46.523357    7508 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0618 16:42:46.571621    7508 docker.go:702] Got preloaded images: -- stdout --
registry.k8s.io/kube-scheduler:v1.33.1
registry.k8s.io/kube-controller-manager:v1.33.1
registry.k8s.io/kube-apiserver:v1.33.1
registry.k8s.io/kube-proxy:v1.33.1
registry.k8s.io/etcd:3.5.21-0
registry.k8s.io/coredns/coredns:v1.12.0
registry.k8s.io/pause:3.10
gcr.io/k8s-minikube/storage-provisioner:v5
nginx:1.14.2

-- /stdout --
I0618 16:42:46.571960    7508 cache_images.go:84] Images are preloaded, skipping loading
I0618 16:42:46.571960    7508 kubeadm.go:926] updating node { 192.168.67.2 8443 v1.33.1 docker true true} ...
I0618 16:42:46.572927    7508 kubeadm.go:938] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.33.1/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.67.2

[Install]
 config:
{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0618 16:42:46.575924    7508 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0618 16:42:46.866091    7508 cni.go:84] Creating CNI manager for ""
I0618 16:42:46.866091    7508 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0618 16:42:46.866614    7508 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0618 16:42:46.866614    7508 kubeadm.go:189] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.67.2 APIServerPort:8443 KubernetesVersion:v1.33.1 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.67.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.67.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0618 16:42:46.866614    7508 kubeadm.go:195] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta4
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.67.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    - name: "node-ip"
      value: "192.168.67.2"
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta4
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.67.2"]
  extraArgs:
    - name: "enable-admission-plugins"
      value: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    - name: "allocate-node-cidrs"
      value: "true"
    - name: "leader-elect"
      value: "false"
scheduler:
  extraArgs:
    - name: "leader-elect"
      value: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      - name: "proxy-refresh-interval"
        value: "70000"
kubernetesVersion: v1.33.1
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%"
  nodefs.inodesFree: "0%"
  imagefs.available: "0%"
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0618 16:42:46.873089    7508 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.33.1
I0618 16:42:46.887292    7508 binaries.go:44] Found k8s binaries, skipping transfer
I0618 16:42:46.893908    7508 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0618 16:42:46.903812    7508 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0618 16:42:46.922696    7508 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0618 16:42:46.943532    7508 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2286 bytes)
I0618 16:42:46.974838    7508 ssh_runner.go:195] Run: grep 192.168.67.2	control-plane.minikube.internal$ /etc/hosts
I0618 16:42:46.980581    7508 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.67.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0618 16:42:46.999246    7508 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0618 16:42:47.106583    7508 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0618 16:42:47.125364    7508 certs.go:68] Setting up C:\Users\Minfy.ADITYAREDDYDARA\.minikube\profiles\minikube for IP: 192.168.67.2
I0618 16:42:47.125871    7508 certs.go:194] generating shared ca certs ...
I0618 16:42:47.125893    7508 certs.go:226] acquiring lock for ca certs: {Name:mk9268bf20760b0ce8540076b199d629d95e3556 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0618 16:42:47.127030    7508 certs.go:235] skipping valid "minikubeCA" ca cert: C:\Users\Minfy.ADITYAREDDYDARA\.minikube\ca.key
I0618 16:42:47.128109    7508 certs.go:235] skipping valid "proxyClientCA" ca cert: C:\Users\Minfy.ADITYAREDDYDARA\.minikube\proxy-client-ca.key
I0618 16:42:47.128109    7508 certs.go:256] generating profile certs ...
I0618 16:42:47.129182    7508 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": C:\Users\Minfy.ADITYAREDDYDARA\.minikube\profiles\minikube\client.key
I0618 16:42:47.130884    7508 certs.go:359] skipping valid signed profile cert regeneration for "minikube": C:\Users\Minfy.ADITYAREDDYDARA\.minikube\profiles\minikube\apiserver.key.583c145e
I0618 16:42:47.132020    7508 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": C:\Users\Minfy.ADITYAREDDYDARA\.minikube\profiles\minikube\proxy-client.key
I0618 16:42:47.132552    7508 certs.go:484] found cert: C:\Users\Minfy.ADITYAREDDYDARA\.minikube\certs\ca-key.pem (1675 bytes)
I0618 16:42:47.133089    7508 certs.go:484] found cert: C:\Users\Minfy.ADITYAREDDYDARA\.minikube\certs\ca.pem (1074 bytes)
I0618 16:42:47.133089    7508 certs.go:484] found cert: C:\Users\Minfy.ADITYAREDDYDARA\.minikube\certs\cert.pem (1119 bytes)
I0618 16:42:47.133089    7508 certs.go:484] found cert: C:\Users\Minfy.ADITYAREDDYDARA\.minikube\certs\key.pem (1675 bytes)
I0618 16:42:47.140241    7508 ssh_runner.go:362] scp C:\Users\Minfy.ADITYAREDDYDARA\.minikube\ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0618 16:42:47.161635    7508 ssh_runner.go:362] scp C:\Users\Minfy.ADITYAREDDYDARA\.minikube\ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I0618 16:42:47.182796    7508 ssh_runner.go:362] scp C:\Users\Minfy.ADITYAREDDYDARA\.minikube\proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0618 16:42:47.203783    7508 ssh_runner.go:362] scp C:\Users\Minfy.ADITYAREDDYDARA\.minikube\proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0618 16:42:47.225491    7508 ssh_runner.go:362] scp C:\Users\Minfy.ADITYAREDDYDARA\.minikube\profiles\minikube\apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0618 16:42:47.246030    7508 ssh_runner.go:362] scp C:\Users\Minfy.ADITYAREDDYDARA\.minikube\profiles\minikube\apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0618 16:42:47.267221    7508 ssh_runner.go:362] scp C:\Users\Minfy.ADITYAREDDYDARA\.minikube\profiles\minikube\proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0618 16:42:47.288782    7508 ssh_runner.go:362] scp C:\Users\Minfy.ADITYAREDDYDARA\.minikube\profiles\minikube\proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I0618 16:42:47.316985    7508 ssh_runner.go:362] scp C:\Users\Minfy.ADITYAREDDYDARA\.minikube\ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0618 16:42:47.337938    7508 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0618 16:42:47.360293    7508 ssh_runner.go:195] Run: openssl version
I0618 16:42:47.385890    7508 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0618 16:42:47.405410    7508 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0618 16:42:47.411201    7508 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 Jun 17 10:55 /usr/share/ca-certificates/minikubeCA.pem
I0618 16:42:47.418708    7508 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0618 16:42:47.436921    7508 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0618 16:42:47.453033    7508 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0618 16:42:47.465097    7508 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0618 16:42:47.480245    7508 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0618 16:42:47.496730    7508 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0618 16:42:47.511704    7508 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0618 16:42:47.528313    7508 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0618 16:42:47.545389    7508 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0618 16:42:47.553182    7508 kubeadm.go:392] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.47@sha256:6ed579c9292b4370177b7ef3c42cc4b4a6dcd0735a1814916cbc22c8bf38412b Memory:2200 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.33.1 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.67.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:C:\Users\Minfy.ADITYAREDDYDARA:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0618 16:42:47.556182    7508 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0618 16:42:47.628354    7508 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I0618 16:42:47.637413    7508 kubeadm.go:408] found existing configuration files, will attempt cluster restart
I0618 16:42:47.637940    7508 kubeadm.go:589] restartPrimaryControlPlane start ...
I0618 16:42:47.645358    7508 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0618 16:42:47.656714    7508 kubeadm.go:130] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0618 16:42:47.660265    7508 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0618 16:42:47.708766    7508 kubeconfig.go:47] verify endpoint returned: get endpoint: "minikube" does not appear in C:\Users\Minfy.ADITYAREDDYDARA\.kube\config
I0618 16:42:47.708766    7508 kubeconfig.go:62] C:\Users\Minfy.ADITYAREDDYDARA\.kube\config needs updating (will repair): [kubeconfig missing "minikube" cluster setting kubeconfig missing "minikube" context setting]
I0618 16:42:47.709766    7508 lock.go:35] WriteFile acquiring C:\Users\Minfy.ADITYAREDDYDARA\.kube\config: {Name:mka4fc914ca9a71e2cf02f3f26e1230144bb5408 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0618 16:42:47.771484    7508 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0618 16:42:47.781176    7508 kubeadm.go:626] The running cluster does not require reconfiguration: 127.0.0.1
I0618 16:42:47.781176    7508 kubeadm.go:593] duration metric: took 143.2358ms to restartPrimaryControlPlane
I0618 16:42:47.781176    7508 kubeadm.go:394] duration metric: took 227.9937ms to StartCluster
I0618 16:42:47.781504    7508 settings.go:142] acquiring lock: {Name:mk24e8318d0b0af6431bf9b716ee25bbc7047e9a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0618 16:42:47.781504    7508 settings.go:150] Updating kubeconfig:  C:\Users\Minfy.ADITYAREDDYDARA\.kube\config
I0618 16:42:47.784643    7508 lock.go:35] WriteFile acquiring C:\Users\Minfy.ADITYAREDDYDARA\.kube\config: {Name:mka4fc914ca9a71e2cf02f3f26e1230144bb5408 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0618 16:42:47.785647    7508 start.go:235] Will wait 6m0s for node &{Name: IP:192.168.67.2 Port:8443 KubernetesVersion:v1.33.1 ContainerRuntime:docker ControlPlane:true Worker:true}
I0618 16:42:47.786162    7508 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.33.1
I0618 16:42:47.788881    7508 out.go:177] * Verifying Kubernetes components...
I0618 16:42:47.786266    7508 addons.go:511] enable addons start: toEnable=map[ambassador:false amd-gpu-device-plugin:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volcano:false volumesnapshots:false yakd:false]
I0618 16:42:47.789357    7508 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0618 16:42:47.789357    7508 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0618 16:42:47.794358    7508 addons.go:238] Setting addon storage-provisioner=true in "minikube"
I0618 16:42:47.794358    7508 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
W0618 16:42:47.794358    7508 addons.go:247] addon storage-provisioner should already be in state true
I0618 16:42:47.794857    7508 host.go:66] Checking if "minikube" exists ...
I0618 16:42:47.804856    7508 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0618 16:42:47.805856    7508 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0618 16:42:47.806866    7508 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0618 16:42:47.863981    7508 out.go:177]   - Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0618 16:42:47.866976    7508 addons.go:435] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0618 16:42:47.866976    7508 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0618 16:42:47.871032    7508 addons.go:238] Setting addon default-storageclass=true in "minikube"
W0618 16:42:47.871032    7508 addons.go:247] addon default-storageclass should already be in state true
I0618 16:42:47.871032    7508 host.go:66] Checking if "minikube" exists ...
I0618 16:42:47.873540    7508 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0618 16:42:47.882539    7508 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0618 16:42:47.931042    7508 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61837 SSHKeyPath:C:\Users\Minfy.ADITYAREDDYDARA\.minikube\machines\minikube\id_rsa Username:docker}
I0618 16:42:47.941099    7508 addons.go:435] installing /etc/kubernetes/addons/storageclass.yaml
I0618 16:42:47.941099    7508 ssh_runner.go:362] scp storageclass/storageclass.yaml --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0618 16:42:47.946099    7508 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0618 16:42:47.994114    7508 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:61837 SSHKeyPath:C:\Users\Minfy.ADITYAREDDYDARA\.minikube\machines\minikube\id_rsa Username:docker}
I0618 16:42:48.170642    7508 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0618 16:42:48.237602    7508 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0618 16:42:48.264780    7508 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0618 16:42:48.296286    7508 api_server.go:52] waiting for apiserver process to appear ...
I0618 16:42:48.303280    7508 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0618 16:42:48.334706    7508 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0618 16:42:48.812819    7508 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0618 16:42:52.153748    7508 ssh_runner.go:235] Completed: sudo pgrep -xnf kube-apiserver.*minikube.*: (3.340593s)
I0618 16:42:52.153748    7508 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (3.8889681s)
W0618 16:42:52.153888    7508 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0618 16:42:52.154244    7508 retry.go:31] will retry after 270.711898ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0618 16:42:52.154244    7508 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (3.819538s)
W0618 16:42:52.154244    7508 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0618 16:42:52.154244    7508 retry.go:31] will retry after 347.934058ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0618 16:42:52.169246    7508 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0618 16:42:52.316303    7508 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0618 16:42:52.446294    7508 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0618 16:42:52.524833    7508 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0618 16:42:52.547885    7508 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0618 16:42:52.547885    7508 retry.go:31] will retry after 419.764034ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0618 16:42:52.806645    7508 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0618 16:42:52.985683    7508 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0618 16:42:53.149615    7508 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0618 16:42:53.149615    7508 retry.go:31] will retry after 236.368188ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0618 16:42:53.308234    7508 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0618 16:42:53.367279    7508 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0618 16:42:53.367279    7508 retry.go:31] will retry after 449.573913ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0618 16:42:53.397157    7508 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0618 16:42:53.583355    7508 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0618 16:42:53.583355    7508 retry.go:31] will retry after 584.172972ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0618 16:42:53.804622    7508 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0618 16:42:53.829611    7508 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0618 16:42:54.179742    7508 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0618 16:42:54.234524    7508 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0618 16:42:54.234524    7508 retry.go:31] will retry after 654.902982ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0618 16:42:54.319092    7508 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0618 16:42:54.531273    7508 api_server.go:72] duration metric: took 6.7452245s to wait for apiserver process to appear ...
I0618 16:42:54.531273    7508 api_server.go:88] waiting for apiserver healthz status ...
W0618 16:42:54.531434    7508 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0618 16:42:54.531566    7508 retry.go:31] will retry after 1.097848031s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0618 16:42:54.531773    7508 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61841/healthz ...
I0618 16:42:54.547875    7508 api_server.go:269] stopped: https://127.0.0.1:61841/healthz: Get "https://127.0.0.1:61841/healthz": EOF
I0618 16:42:54.898785    7508 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0618 16:42:55.032305    7508 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61841/healthz ...
I0618 16:42:55.036740    7508 api_server.go:269] stopped: https://127.0.0.1:61841/healthz: Get "https://127.0.0.1:61841/healthz": EOF
W0618 16:42:55.247889    7508 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0618 16:42:55.247889    7508 retry.go:31] will retry after 1.260422011s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0618 16:42:55.532266    7508 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61841/healthz ...
I0618 16:42:55.536464    7508 api_server.go:269] stopped: https://127.0.0.1:61841/healthz: Get "https://127.0.0.1:61841/healthz": EOF
I0618 16:42:55.656920    7508 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0618 16:42:56.032375    7508 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61841/healthz ...
I0618 16:42:56.036323    7508 api_server.go:269] stopped: https://127.0.0.1:61841/healthz: Get "https://127.0.0.1:61841/healthz": EOF
W0618 16:42:56.118523    7508 addons.go:461] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0618 16:42:56.118939    7508 retry.go:31] will retry after 1.723822747s: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp [::1]:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0618 16:42:56.531907    7508 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61841/healthz ...
I0618 16:42:56.537118    7508 api_server.go:269] stopped: https://127.0.0.1:61841/healthz: Get "https://127.0.0.1:61841/healthz": EOF
I0618 16:42:56.538535    7508 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0618 16:42:57.031578    7508 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61841/healthz ...
I0618 16:42:57.860724    7508 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0618 16:43:01.254354    7508 api_server.go:279] https://127.0.0.1:61841/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0618 16:43:01.254831    7508 api_server.go:103] status: https://127.0.0.1:61841/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0618 16:43:01.254831    7508 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61841/healthz ...
I0618 16:43:01.429785    7508 api_server.go:279] https://127.0.0.1:61841/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\": RBAC: clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found","reason":"Forbidden","details":{},"code":403}
W0618 16:43:01.429785    7508 api_server.go:103] status: https://127.0.0.1:61841/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\": RBAC: clusterrole.rbac.authorization.k8s.io \"system:public-info-viewer\" not found","reason":"Forbidden","details":{},"code":403}
I0618 16:43:01.532051    7508 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61841/healthz ...
I0618 16:43:01.632093    7508 api_server.go:279] https://127.0.0.1:61841/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0618 16:43:01.632093    7508 api_server.go:103] status: https://127.0.0.1:61841/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0618 16:43:02.031591    7508 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61841/healthz ...
I0618 16:43:02.041692    7508 api_server.go:279] https://127.0.0.1:61841/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0618 16:43:02.041692    7508 api_server.go:103] status: https://127.0.0.1:61841/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0618 16:43:02.531837    7508 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61841/healthz ...
I0618 16:43:02.549088    7508 api_server.go:279] https://127.0.0.1:61841/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0618 16:43:02.549613    7508 api_server.go:103] status: https://127.0.0.1:61841/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-kubernetes-service-cidr-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-status-local-available-controller ok
[+]poststarthook/apiservice-status-remote-available-controller ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0618 16:43:03.031639    7508 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:61841/healthz ...
I0618 16:43:03.040876    7508 api_server.go:279] https://127.0.0.1:61841/healthz returned 200:
ok
I0618 16:43:03.042647    7508 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (5.1819237s)
I0618 16:43:03.042647    7508 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.33.1/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (6.5041119s)
I0618 16:43:03.058054    7508 api_server.go:141] control plane version: v1.33.1
I0618 16:43:03.058054    7508 api_server.go:131] duration metric: took 8.5267807s to wait for apiserver health ...
I0618 16:43:03.058536    7508 system_pods.go:43] waiting for kube-system pods to appear ...
I0618 16:43:03.073800    7508 out.go:177] * Enabled addons: storage-provisioner, default-storageclass
I0618 16:43:03.076415    7508 addons.go:514] duration metric: took 15.2912523s for enable addons: enabled=[storage-provisioner default-storageclass]
I0618 16:43:03.082237    7508 system_pods.go:59] 7 kube-system pods found
I0618 16:43:03.082237    7508 system_pods.go:61] "coredns-674b8bbfcf-2677m" [4c9bd85e-050b-475e-a6b5-a3aa42948e50] Running
I0618 16:43:03.082237    7508 system_pods.go:61] "etcd-minikube" [2e27b543-8e7e-46f3-90fc-ea5bf81c0d00] Running
I0618 16:43:03.082237    7508 system_pods.go:61] "kube-apiserver-minikube" [663b02c9-3dda-468a-ad30-b550fbf08739] Running
I0618 16:43:03.082237    7508 system_pods.go:61] "kube-controller-manager-minikube" [e500facb-e648-4fc6-b6b3-ac0049f67beb] Running
I0618 16:43:03.082237    7508 system_pods.go:61] "kube-proxy-vvmv2" [2c4d0aa8-5307-48a1-8374-160c5ab9e3dc] Running
I0618 16:43:03.082237    7508 system_pods.go:61] "kube-scheduler-minikube" [35a71320-7827-4fb2-8605-3654ebc51b08] Running
I0618 16:43:03.082237    7508 system_pods.go:61] "storage-provisioner" [7e2a5729-b847-4ac2-ac4d-3b613f3ba0db] Running
I0618 16:43:03.082237    7508 system_pods.go:74] duration metric: took 23.7005ms to wait for pod list to return data ...
I0618 16:43:03.082237    7508 kubeadm.go:578] duration metric: took 15.2965893s to wait for: map[apiserver:true system_pods:true]
I0618 16:43:03.082237    7508 node_conditions.go:102] verifying NodePressure condition ...
I0618 16:43:03.086728    7508 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0618 16:43:03.086728    7508 node_conditions.go:123] node cpu capacity is 8
I0618 16:43:03.087251    7508 node_conditions.go:105] duration metric: took 5.0141ms to run NodePressure ...
I0618 16:43:03.087251    7508 start.go:241] waiting for startup goroutines ...
I0618 16:43:03.087251    7508 start.go:246] waiting for cluster config update ...
I0618 16:43:03.087251    7508 start.go:255] writing updated cluster config ...
I0618 16:43:03.094889    7508 ssh_runner.go:195] Run: rm -f paused
I0618 16:43:03.106105    7508 out.go:177] * kubectl not found. If you need it, try: 'minikube kubectl -- get pods -A'
I0618 16:43:03.109320    7508 out.go:177] * Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
Jun 18 11:21:50 minikube cri-dockerd[1539]: time="2025-06-18T11:21:50Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Jun 18 11:22:08 minikube cri-dockerd[1539]: time="2025-06-18T11:22:08Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Jun 18 11:22:24 minikube cri-dockerd[1539]: time="2025-06-18T11:22:24Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Jun 18 11:22:38 minikube cri-dockerd[1539]: time="2025-06-18T11:22:38Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Jun 18 11:22:43 minikube cri-dockerd[1539]: time="2025-06-18T11:22:43Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ab56e369a93bdb408adc13232a196f84f1b863bd80f74805caebddfc3c782002/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 18 11:22:57 minikube cri-dockerd[1539]: time="2025-06-18T11:22:57Z" level=info msg="Pulling image mongo-express:latest: 7e9a007eb24b: Extracting [============>                                      ]  10.22MB/39.7MB"
Jun 18 11:23:07 minikube cri-dockerd[1539]: time="2025-06-18T11:23:07Z" level=info msg="Pulling image mongo-express:latest: 9f7f59574f7d: Extracting [=============================================>     ]  12.45MB/13.64MB"
Jun 18 11:23:08 minikube cri-dockerd[1539]: time="2025-06-18T11:23:08Z" level=info msg="Stop pulling image mongo-express:latest: Status: Downloaded newer image for mongo-express:latest"
Jun 18 11:23:09 minikube dockerd[1198]: time="2025-06-18T11:23:09.492964727Z" level=info msg="ignoring event" container=ab56e369a93bdb408adc13232a196f84f1b863bd80f74805caebddfc3c782002 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 18 11:23:11 minikube cri-dockerd[1539]: time="2025-06-18T11:23:11Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ea7186a86c4f75eda990ad603a4b3ef6363bb656b960922f28f8dbcd08e68055/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 18 11:23:11 minikube cri-dockerd[1539]: time="2025-06-18T11:23:11Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Jun 18 11:23:14 minikube cri-dockerd[1539]: time="2025-06-18T11:23:14Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jun 18 11:23:18 minikube cri-dockerd[1539]: time="2025-06-18T11:23:18Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jun 18 11:23:29 minikube cri-dockerd[1539]: time="2025-06-18T11:23:29Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Jun 18 11:23:32 minikube cri-dockerd[1539]: time="2025-06-18T11:23:32Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jun 18 11:23:44 minikube cri-dockerd[1539]: time="2025-06-18T11:23:44Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Jun 18 11:23:47 minikube cri-dockerd[1539]: time="2025-06-18T11:23:47Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jun 18 11:24:01 minikube cri-dockerd[1539]: time="2025-06-18T11:24:01Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Jun 18 11:24:05 minikube cri-dockerd[1539]: time="2025-06-18T11:24:05Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jun 18 11:24:16 minikube cri-dockerd[1539]: time="2025-06-18T11:24:16Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Jun 18 11:24:19 minikube cri-dockerd[1539]: time="2025-06-18T11:24:19Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jun 18 11:24:30 minikube cri-dockerd[1539]: time="2025-06-18T11:24:30Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Jun 18 11:24:37 minikube cri-dockerd[1539]: time="2025-06-18T11:24:37Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jun 18 11:24:46 minikube cri-dockerd[1539]: time="2025-06-18T11:24:46Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Jun 18 11:24:50 minikube cri-dockerd[1539]: time="2025-06-18T11:24:50Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jun 18 11:25:04 minikube cri-dockerd[1539]: time="2025-06-18T11:25:04Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Jun 18 11:25:07 minikube cri-dockerd[1539]: time="2025-06-18T11:25:07Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jun 18 11:25:22 minikube cri-dockerd[1539]: time="2025-06-18T11:25:22Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Jun 18 11:25:25 minikube cri-dockerd[1539]: time="2025-06-18T11:25:25Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jun 18 11:25:36 minikube cri-dockerd[1539]: time="2025-06-18T11:25:36Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Jun 18 11:25:39 minikube cri-dockerd[1539]: time="2025-06-18T11:25:39Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jun 18 11:25:52 minikube cri-dockerd[1539]: time="2025-06-18T11:25:52Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jun 18 11:25:55 minikube cri-dockerd[1539]: time="2025-06-18T11:25:55Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Jun 18 11:26:02 minikube dockerd[1198]: time="2025-06-18T11:26:02.071191963Z" level=info msg="ignoring event" container=ea7186a86c4f75eda990ad603a4b3ef6363bb656b960922f28f8dbcd08e68055 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 18 11:26:11 minikube cri-dockerd[1539]: time="2025-06-18T11:26:11Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Jun 18 11:26:26 minikube cri-dockerd[1539]: time="2025-06-18T11:26:26Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Jun 18 11:26:29 minikube dockerd[1198]: time="2025-06-18T11:26:29.202814242Z" level=info msg="ignoring event" container=5bf20b15327c563b3a11a56e647d032b2ca2d6023519347f8572fefa8fa45933 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Jun 18 11:28:56 minikube cri-dockerd[1539]: time="2025-06-18T11:28:56Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/af4b7796a6219a39531a36c44f74bee25b9f579aebaddd237739293dc80fad9c/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 18 11:28:58 minikube cri-dockerd[1539]: time="2025-06-18T11:28:58Z" level=info msg="Stop pulling image mongo:latest: Status: Image is up to date for mongo:latest"
Jun 18 11:29:24 minikube cri-dockerd[1539]: time="2025-06-18T11:29:24Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c8a33117cf6ee73807598c59ae5fcd525fe97072b0a93aacf9d29c77563c3abe/resolv.conf as [nameserver 10.96.0.10 search default.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Jun 18 11:29:27 minikube cri-dockerd[1539]: time="2025-06-18T11:29:27Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jun 18 11:29:31 minikube cri-dockerd[1539]: time="2025-06-18T11:29:31Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jun 18 11:29:44 minikube cri-dockerd[1539]: time="2025-06-18T11:29:44Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jun 18 11:30:02 minikube cri-dockerd[1539]: time="2025-06-18T11:30:02Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jun 18 11:30:18 minikube cri-dockerd[1539]: time="2025-06-18T11:30:18Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jun 18 11:30:33 minikube cri-dockerd[1539]: time="2025-06-18T11:30:33Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jun 18 11:30:51 minikube cri-dockerd[1539]: time="2025-06-18T11:30:51Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jun 18 11:31:06 minikube cri-dockerd[1539]: time="2025-06-18T11:31:06Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jun 18 11:31:22 minikube cri-dockerd[1539]: time="2025-06-18T11:31:22Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jun 18 11:31:39 minikube cri-dockerd[1539]: time="2025-06-18T11:31:39Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jun 18 11:31:55 minikube cri-dockerd[1539]: time="2025-06-18T11:31:55Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jun 18 11:32:13 minikube cri-dockerd[1539]: time="2025-06-18T11:32:13Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jun 18 11:32:27 minikube cri-dockerd[1539]: time="2025-06-18T11:32:27Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jun 18 11:32:45 minikube cri-dockerd[1539]: time="2025-06-18T11:32:45Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jun 18 11:33:00 minikube cri-dockerd[1539]: time="2025-06-18T11:33:00Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jun 18 11:33:19 minikube cri-dockerd[1539]: time="2025-06-18T11:33:19Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jun 18 11:33:34 minikube cri-dockerd[1539]: time="2025-06-18T11:33:34Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jun 18 11:33:52 minikube cri-dockerd[1539]: time="2025-06-18T11:33:52Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jun 18 11:34:06 minikube cri-dockerd[1539]: time="2025-06-18T11:34:06Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"
Jun 18 11:34:21 minikube cri-dockerd[1539]: time="2025-06-18T11:34:21Z" level=info msg="Stop pulling image mongo-express:latest: Status: Image is up to date for mongo-express:latest"


==> container status <==
CONTAINER           IMAGE                                                                           CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
c985c5a161faf       mongo@sha256:98028cf281bb5d49ace5e1ddbd4509e8f1382fe80ef1cf101eeefdc106d76cd4   5 minutes ago       Running             mongodb                   0                   af4b7796a6219       mongodb-deployment-6d9d7c68f6-cr8vm
d00e68b432f1b       6e38f40d628db                                                                   21 minutes ago      Running             storage-provisioner       6                   6aa6255df4d91       storage-provisioner
5cb1ca99808b9       295c7be079025                                                                   21 minutes ago      Running             nginx                     1                   2620ae3b36d65       nginx-deployment-647677fc66-q5kfx
acc9ce7e7ce83       1cf5f116067c6                                                                   21 minutes ago      Running             coredns                   2                   50f588d930fa6       coredns-674b8bbfcf-2677m
243877ae9d33b       295c7be079025                                                                   21 minutes ago      Running             nginx                     1                   d0246bea17307       nginx-deployment-647677fc66-5dflt
8518f3912589e       295c7be079025                                                                   21 minutes ago      Running             nginx-container           2                   8a2ddba9f5a89       nginx-pod
84e3b335ffe21       6e38f40d628db                                                                   21 minutes ago      Exited              storage-provisioner       5                   6aa6255df4d91       storage-provisioner
7c775dc58144d       b79c189b052cd                                                                   21 minutes ago      Running             kube-proxy                2                   c81e0dfd5a67f       kube-proxy-vvmv2
110afad834228       c6ab243b29f82                                                                   21 minutes ago      Running             kube-apiserver            2                   c1fcd0d05b520       kube-apiserver-minikube
48a9dbeb83b95       398c985c0d950                                                                   21 minutes ago      Running             kube-scheduler            2                   fffc30b890376       kube-scheduler-minikube
a5a7832dce37a       ef43894fa110c                                                                   21 minutes ago      Running             kube-controller-manager   2                   895d6ca17d20b       kube-controller-manager-minikube
4ecfa42c5eab7       499038711c081                                                                   21 minutes ago      Running             etcd                      2                   ec891abf26ce4       etcd-minikube
18e87aa6186a9       295c7be079025                                                                   53 minutes ago      Exited              nginx                     0                   5d48952e35216       nginx-deployment-647677fc66-q5kfx
24252ea46b0cb       295c7be079025                                                                   53 minutes ago      Exited              nginx                     0                   e4c726c9bfd69       nginx-deployment-647677fc66-5dflt
40ab5f5f84f66       1cf5f116067c6                                                                   55 minutes ago      Exited              coredns                   1                   e3ac1abea03be       coredns-674b8bbfcf-2677m
54702b2d3e691       295c7be079025                                                                   55 minutes ago      Exited              nginx-container           1                   75a7f15368f79       nginx-pod
b34889263aec3       b79c189b052cd                                                                   55 minutes ago      Exited              kube-proxy                1                   cb66f4e123187       kube-proxy-vvmv2
a6344c6c6e601       c6ab243b29f82                                                                   56 minutes ago      Exited              kube-apiserver            1                   67a16a419b27e       kube-apiserver-minikube
2cdb01faa4921       ef43894fa110c                                                                   56 minutes ago      Exited              kube-controller-manager   1                   e9aace7d78446       kube-controller-manager-minikube
50a8afab30f2b       398c985c0d950                                                                   56 minutes ago      Exited              kube-scheduler            1                   b6eef38c4c2bf       kube-scheduler-minikube
3c8c0efd5f884       499038711c081                                                                   56 minutes ago      Exited              etcd                      1                   4a3e25ac2d0fd       etcd-minikube


==> coredns [40ab5f5f84f6] <==
maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] 127.0.0.1:33575 - 34190 "HINFO IN 149516005748816966.2875438499323365941. udp 56 false 512" NXDOMAIN qr,rd,ra 56 0.116896032s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> coredns [acc9ce7e7ce8] <==
maxprocs: Leaving GOMAXPROCS=8: CPU quota undefined
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.31.2/tools/cache/reflector.go:243: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": tls: failed to verify certificate: x509: certificate signed by unknown authority
[ERROR] plugin/kubernetes: Unhandled Error
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
.:53
[INFO] plugin/reload: Running configuration SHA512 = e7e8a6c4578bf29b9f453cb54ade3fb14671793481527b7435e35119b25e84eb3a79242b1f470199f8605ace441674db8f1b6715b77448c20dde63e2dc5d2169
CoreDNS-1.12.0
linux/amd64, go1.23.3, 51e11f1
[INFO] 127.0.0.1:60144 - 12895 "HINFO IN 3675671460058819824.5768640686153596702. udp 57 false 512" NXDOMAIN qr,rd,ra 57 0.118374282s
[INFO] 10.244.0.15:36717 - 2341 "A IN compass.mongodb.com.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.00112394s
[INFO] 10.244.0.15:43794 - 42104 "A IN compass.mongodb.com.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.000590846s
[INFO] 10.244.0.15:43794 - 16485 "AAAA IN compass.mongodb.com.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.001454373s
[INFO] 10.244.0.15:36717 - 6186 "AAAA IN compass.mongodb.com.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.00126543s
[INFO] 10.244.0.15:55774 - 24118 "A IN compass.mongodb.com.default.svc.cluster.local. udp 63 false 512" NXDOMAIN qr,aa,rd 156 0.038097699s
[INFO] 10.244.0.15:55774 - 27179 "AAAA IN compass.mongodb.com.default.svc.cluster.local. udp 63 false 512" NXDOMAIN qr,aa,rd 156 0.038065073s
[INFO] 10.244.0.15:33880 - 8051 "AAAA IN compass.mongodb.com. udp 37 false 512" NOERROR qr,rd,ra 96 0.097054553s
[INFO] 10.244.0.15:33880 - 47729 "A IN compass.mongodb.com. udp 37 false 512" NOERROR qr,rd,ra 516 0.097879896s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=f8f52f5de11fc6ad8244afac475e1d0f96841df1-dirty
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2025_06_17T16_34_23_0700
                    minikube.k8s.io/version=v1.36.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 17 Jun 2025 11:04:19 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Wed, 18 Jun 2025 11:34:27 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Wed, 18 Jun 2025 11:33:37 +0000   Tue, 17 Jun 2025 11:04:18 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Wed, 18 Jun 2025 11:33:37 +0000   Tue, 17 Jun 2025 11:04:18 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Wed, 18 Jun 2025 11:33:37 +0000   Tue, 17 Jun 2025 11:04:18 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Wed, 18 Jun 2025 11:33:37 +0000   Tue, 17 Jun 2025 11:04:20 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.67.2
  Hostname:    minikube
Capacity:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3926808Ki
  pods:               110
Allocatable:
  cpu:                8
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             3926808Ki
  pods:               110
System Info:
  Machine ID:                 1cdc05815f8a493aae86b4c7df9c399d
  System UUID:                1cdc05815f8a493aae86b4c7df9c399d
  Boot ID:                    28450f01-8e57-4b5f-ab62-89ded7e493d4
  Kernel Version:             5.15.167.4-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://28.1.1
  Kubelet Version:            v1.33.1
  Kube-Proxy Version:         
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (12 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  default                     mongo-express-deployment-6965d87ff4-9sfml    0 (0%)        0 (0%)      0 (0%)           0 (0%)         5m12s
  default                     mongodb-deployment-6d9d7c68f6-cr8vm          0 (0%)        0 (0%)      0 (0%)           0 (0%)         5m39s
  default                     nginx-deployment-647677fc66-5dflt            0 (0%)        0 (0%)      0 (0%)           0 (0%)         53m
  default                     nginx-deployment-647677fc66-q5kfx            0 (0%)        0 (0%)      0 (0%)           0 (0%)         53m
  default                     nginx-pod                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         24h
  kube-system                 coredns-674b8bbfcf-2677m                     100m (1%)     0 (0%)      70Mi (1%)        170Mi (4%)     24h
  kube-system                 etcd-minikube                                100m (1%)     0 (0%)      100Mi (2%)       0 (0%)         24h
  kube-system                 kube-apiserver-minikube                      250m (3%)     0 (0%)      0 (0%)           0 (0%)         24h
  kube-system                 kube-controller-manager-minikube             200m (2%)     0 (0%)      0 (0%)           0 (0%)         24h
  kube-system                 kube-proxy-vvmv2                             0 (0%)        0 (0%)      0 (0%)           0 (0%)         24h
  kube-system                 kube-scheduler-minikube                      100m (1%)     0 (0%)      0 (0%)           0 (0%)         24h
  kube-system                 storage-provisioner                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         24h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (9%)   0 (0%)
  memory             170Mi (4%)  170Mi (4%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-1Gi      0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                             Age                From             Message
  ----     ------                             ----               ----             -------
  Normal   Starting                           24h                kube-proxy       
  Normal   Starting                           21m                kube-proxy       
  Normal   Starting                           55m                kube-proxy       
  Normal   NodeAllocatableEnforced            24h                kubelet          Updated Node Allocatable limit across pods
  Normal   Starting                           24h                kubelet          Starting kubelet.
  Warning  CgroupV1                           24h                kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeHasSufficientMemory            24h                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              24h                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               24h                kubelet          Node minikube status is now: NodeHasSufficientPID
  Warning  PossibleMemoryBackedVolumesOnDisk  24h                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   RegisteredNode                     24h                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal   NodeAllocatableEnforced            56m                kubelet          Updated Node Allocatable limit across pods
  Warning  PossibleMemoryBackedVolumesOnDisk  56m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           56m                kubelet          Starting kubelet.
  Warning  CgroupV1                           56m                kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeHasSufficientMemory            56m (x8 over 56m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasSufficientPID               56m (x7 over 56m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeHasNoDiskPressure              56m (x8 over 56m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Warning  Rebooted                           56m                kubelet          Node minikube has been rebooted, boot id: 28450f01-8e57-4b5f-ab62-89ded7e493d4
  Normal   RegisteredNode                     55m                node-controller  Node minikube event: Registered Node minikube in Controller
  Warning  PossibleMemoryBackedVolumesOnDisk  21m                kubelet          The tmpfs noswap option is not supported. Memory-backed volumes (e.g. secrets, emptyDirs, etc.) might be swapped to disk and should no longer be considered secure.
  Normal   Starting                           21m                kubelet          Starting kubelet.
  Warning  CgroupV1                           21m                kubelet          cgroup v1 support is in maintenance mode, please migrate to cgroup v2
  Normal   NodeHasSufficientMemory            21m (x8 over 21m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure              21m (x8 over 21m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID               21m (x7 over 21m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced            21m                kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode                     21m                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[  +0.000344] FS-Cache: N-cookie d=000000001f3ea201{9P.session} n=000000000ae6b731
[  +0.001041] FS-Cache: N-key=[10] '34323934393337343131'
[  +0.008827] WSL (1 - init(docker-desktop)) WARNING: /usr/share/zoneinfo/Asia/Calcutta not found. Is the tzdata package installed?
[  +0.488851] misc dxg: dxgk: dxgkio_is_feature_enabled: Ioctl failed: -22
[  +0.022269] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.002052] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000791] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.001562] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +3.910054] WSL (156) ERROR: CheckConnection: getaddrinfo() failed: -5
[  +1.929570] netlink: 'init': attribute type 4 has an invalid length.
[Jun18 09:01] tmpfs: Unknown parameter 'noswap'
[ +35.391917] hrtimer: interrupt took 93770457 ns
[Jun18 09:50] rcu: INFO: rcu_sched detected stalls on CPUs/tasks:
[  +0.057874] rcu: 	0-...!: (1 GPs behind) idle=7f5/0/0x3 softirq=91495/91496 fqs=1 
[  +0.089323] rcu: 	1-...!: (1 GPs behind) idle=f2f/0/0x1 softirq=80703/80704 fqs=2 
[  +0.224458] rcu: 	6-...!: (1 GPs behind) idle=c5a/0/0x0 softirq=91095/91110 fqs=2 
[  +0.159974] rcu: 	7-...!: (1 ticks this GP) idle=10f/1/0x4000000000000000 softirq=82669/82670 fqs=2 
[  +0.075560] 	(detected by 1, t=15230 jiffies, g=250457, q=1603)
[  +0.034882] NMI backtrace for cpu 1
[  +0.018339] CPU: 1 PID: 0 Comm: swapper/1 Not tainted 5.15.167.4-microsoft-standard-WSL2 #1
[  +0.035169] Call Trace:
[  +0.033136]  <IRQ>
[  +0.018856]  dump_stack_lvl+0x34/0x48
[  +0.037376]  nmi_cpu_backtrace.cold+0x30/0x70
[  +0.008354]  ? lapic_can_unplug_cpu+0x80/0x80
[  +0.065241]  nmi_trigger_cpumask_backtrace+0xcd/0xd0
[  +0.022133]  rcu_dump_cpu_stacks+0xc1/0xf3
[  +0.022877]  rcu_sched_clock_irq.cold+0x44/0x220
[  +0.125047]  update_process_times+0x8c/0xc0
[  +0.087180]  tick_sched_timer+0x8c/0xa0
[  +0.016687]  ? tick_sched_do_timer+0x90/0x90
[  +0.014462]  __hrtimer_run_queues+0x122/0x270
[  +0.000592]  hrtimer_interrupt+0x10e/0x240
[  +0.006339]  __sysvec_hyperv_stimer0+0x2b/0x60
[  +0.004571]  sysvec_hyperv_stimer0+0x6d/0x90
[  +0.002684]  </IRQ>
[  +0.000771]  <TASK>
[  +0.008385]  asm_sysvec_hyperv_stimer0+0x16/0x20
[  +0.007646] RIP: 0010:default_idle+0x10/0x20
[  +0.000679] Code: cc 0f ae f0 0f ae 38 0f ae f0 eb b5 66 66 2e 0f 1f 84 00 00 00 00 00 0f 1f 00 0f 1f 44 00 00 eb 07 0f 00 2d 92 44 4e 00 fb f4 <c3> cc cc cc cc cc cc cc cc cc cc cc cc cc cc cc 0f 1f 44 00 00 65
[  +0.098989] RSP: 0018:ffff9fe3400abef8 EFLAGS: 00000202
[  +0.036232] RAX: ffffffff87f31f10 RBX: ffff8e26812d0000 RCX: 0000000000000000
[  +0.023541] RDX: 000000000046cf2b RSI: ffff9fe3400abe88 RDI: 000000000046cf2c
[  +0.011758] RBP: 0000000000000001 R08: 0000000000000001 R09: 000000000000019f
[  +0.005744] R10: 0000000000000001 R11: 0000000000000000 R12: 0000000000000000
[  +0.001652] R13: 0000000000000000 R14: 0000000000000000 R15: 0000000000000000
[  +0.001844]  ? mwait_idle+0x80/0x80
[  +0.000666]  default_idle_call+0x33/0xb0
[  +0.000525]  do_idle+0x1ea/0x220
[  +0.006315]  cpu_startup_entry+0x19/0x20
[  +0.003896]  secondary_startup_64_no_verify+0xb0/0xbb
[  +0.002762]  </TASK>
[  +7.450095] WSL (156) ERROR: CheckConnection: getaddrinfo() failed: -5
[Jun18 10:05] WSL (156) ERROR: CheckConnection: getaddrinfo() failed: -5
[Jun18 10:22] hv_utils: TIMESYNC IC: Stale time stamp, 764575755300 nsecs old
[  +8.095239] WSL (156) ERROR: CheckConnection: getaddrinfo() failed: -5
[Jun18 10:36] WSL (156) ERROR: CheckConnection: getaddrinfo() failed: -5
[Jun18 10:38] tmpfs: Unknown parameter 'noswap'
[Jun18 11:07] WSL (156) ERROR: CheckConnection: getaddrinfo() failed: -5
[Jun18 11:12] tmpfs: Unknown parameter 'noswap'


==> etcd [3c8c0efd5f88] <==
{"level":"info","ts":"2025-06-18T10:38:21.976299Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"8688e899f7831fc7","timeout":"7s"}
{"level":"info","ts":"2025-06-18T10:38:21.976821Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"8688e899f7831fc7"}
{"level":"info","ts":"2025-06-18T10:38:21.976888Z","caller":"etcdserver/server.go:875","msg":"starting etcd server","local-member-id":"8688e899f7831fc7","local-server-version":"3.5.21","cluster-version":"to_be_decided"}
{"level":"info","ts":"2025-06-18T10:38:21.977154Z","caller":"etcdserver/server.go:775","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2025-06-18T10:38:21.977304Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2025-06-18T10:38:21.988596Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2025-06-18T10:38:21.988702Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2025-06-18T10:38:22.005143Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"8688e899f7831fc7 switched to configuration voters=(9694253945895198663)"}
{"level":"info","ts":"2025-06-18T10:38:22.005299Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"9d8fdeb88b6def78","local-member-id":"8688e899f7831fc7","added-peer-id":"8688e899f7831fc7","added-peer-peer-urls":["https://192.168.67.2:2380"],"added-peer-is-learner":false}
{"level":"info","ts":"2025-06-18T10:38:22.005489Z","caller":"membership/cluster.go:587","msg":"set initial cluster version","cluster-id":"9d8fdeb88b6def78","local-member-id":"8688e899f7831fc7","cluster-version":"3.5"}
{"level":"info","ts":"2025-06-18T10:38:22.005603Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2025-06-18T10:38:22.027573Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-06-18T10:38:22.036076Z","caller":"embed/etcd.go:762","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2025-06-18T10:38:22.036383Z","caller":"embed/etcd.go:633","msg":"serving peer traffic","address":"192.168.67.2:2380"}
{"level":"info","ts":"2025-06-18T10:38:22.036412Z","caller":"embed/etcd.go:603","msg":"cmux::serve","address":"192.168.67.2:2380"}
{"level":"info","ts":"2025-06-18T10:38:22.037370Z","caller":"embed/etcd.go:292","msg":"now serving peer/client/metrics","local-member-id":"8688e899f7831fc7","initial-advertise-peer-urls":["https://192.168.67.2:2380"],"listen-peer-urls":["https://192.168.67.2:2380"],"advertise-client-urls":["https://192.168.67.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.67.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2025-06-18T10:38:22.037421Z","caller":"embed/etcd.go:908","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2025-06-18T10:38:23.629342Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"8688e899f7831fc7 is starting a new election at term 2"}
{"level":"info","ts":"2025-06-18T10:38:23.629524Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"8688e899f7831fc7 became pre-candidate at term 2"}
{"level":"info","ts":"2025-06-18T10:38:23.629604Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"8688e899f7831fc7 received MsgPreVoteResp from 8688e899f7831fc7 at term 2"}
{"level":"info","ts":"2025-06-18T10:38:23.629639Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"8688e899f7831fc7 became candidate at term 3"}
{"level":"info","ts":"2025-06-18T10:38:23.629915Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"8688e899f7831fc7 received MsgVoteResp from 8688e899f7831fc7 at term 3"}
{"level":"info","ts":"2025-06-18T10:38:23.629951Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"8688e899f7831fc7 became leader at term 3"}
{"level":"info","ts":"2025-06-18T10:38:23.629995Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: 8688e899f7831fc7 elected leader 8688e899f7831fc7 at term 3"}
{"level":"info","ts":"2025-06-18T10:38:23.649026Z","caller":"etcdserver/server.go:2144","msg":"published local member to cluster through raft","local-member-id":"8688e899f7831fc7","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.67.2:2379]}","request-path":"/0/members/8688e899f7831fc7/attributes","cluster-id":"9d8fdeb88b6def78","publish-timeout":"7s"}
{"level":"info","ts":"2025-06-18T10:38:23.649041Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-06-18T10:38:23.649079Z","caller":"embed/serve.go:124","msg":"ready to serve client requests"}
{"level":"info","ts":"2025-06-18T10:38:23.649936Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2025-06-18T10:38:23.650720Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2025-06-18T10:38:23.659331Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-06-18T10:38:23.662443Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.67.2:2379"}
{"level":"info","ts":"2025-06-18T10:38:23.668207Z","caller":"v3rpc/health.go:61","msg":"grpc service status changed","service":"","status":"SERVING"}
{"level":"info","ts":"2025-06-18T10:38:23.668984Z","caller":"embed/serve.go:275","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"warn","ts":"2025-06-18T10:38:31.291509Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"104.112113ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" limit:1 ","response":"range_response_count:1 size:686"}
{"level":"info","ts":"2025-06-18T10:38:31.291916Z","caller":"traceutil/trace.go:171","msg":"trace[1803429734] range","detail":"{range_begin:/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii; range_end:; response_count:1; response_revision:688; }","duration":"105.232537ms","start":"2025-06-18T10:38:31.186492Z","end":"2025-06-18T10:38:31.291725Z","steps":["trace[1803429734] 'agreement among raft nodes before linearized reading'  (duration: 100.60506ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-18T10:38:32.874318Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"104.360129ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterroles/system:auth-delegator\" limit:1 ","response":"range_response_count:1 size:651"}
{"level":"info","ts":"2025-06-18T10:38:32.874558Z","caller":"traceutil/trace.go:171","msg":"trace[244158344] range","detail":"{range_begin:/registry/clusterroles/system:auth-delegator; range_end:; response_count:1; response_revision:720; }","duration":"104.6068ms","start":"2025-06-18T10:38:32.769864Z","end":"2025-06-18T10:38:32.874471Z","steps":["trace[244158344] 'range keys from in-memory index tree'  (duration: 104.139334ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-18T10:38:32.874975Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"100.997677ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-06-18T10:38:32.875066Z","caller":"traceutil/trace.go:171","msg":"trace[942210060] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:720; }","duration":"101.19486ms","start":"2025-06-18T10:38:32.773845Z","end":"2025-06-18T10:38:32.875040Z","steps":["trace[942210060] 'range keys from in-memory index tree'  (duration: 100.978645ms)"],"step_count":1}
{"level":"info","ts":"2025-06-18T10:38:33.789283Z","caller":"traceutil/trace.go:171","msg":"trace[1748593227] transaction","detail":"{read_only:false; response_revision:735; number_of_response:1; }","duration":"316.537886ms","start":"2025-06-18T10:38:33.472728Z","end":"2025-06-18T10:38:33.789266Z","steps":["trace[1748593227] 'process raft request'  (duration: 296.685992ms)","trace[1748593227] 'compare'  (duration: 19.676866ms)"],"step_count":2}
{"level":"warn","ts":"2025-06-18T10:38:33.789357Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"314.433044ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterroles/system:controller:disruption-controller\" limit:1 ","response":"range_response_count:1 size:972"}
{"level":"info","ts":"2025-06-18T10:38:33.789388Z","caller":"traceutil/trace.go:171","msg":"trace[1222497857] range","detail":"{range_begin:/registry/clusterroles/system:controller:disruption-controller; range_end:; response_count:1; response_revision:735; }","duration":"314.515148ms","start":"2025-06-18T10:38:33.474867Z","end":"2025-06-18T10:38:33.789382Z","steps":["trace[1222497857] 'agreement among raft nodes before linearized reading'  (duration: 314.429679ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-18T10:38:33.789457Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-06-18T10:38:33.474850Z","time spent":"314.549398ms","remote":"127.0.0.1:49802","response type":"/etcdserverpb.KV/Range","request count":0,"request size":66,"response count":1,"response size":996,"request content":"key:\"/registry/clusterroles/system:controller:disruption-controller\" limit:1 "}
{"level":"info","ts":"2025-06-18T10:38:33.789242Z","caller":"traceutil/trace.go:171","msg":"trace[77173942] linearizableReadLoop","detail":"{readStateIndex:826; appliedIndex:825; }","duration":"314.294443ms","start":"2025-06-18T10:38:33.474907Z","end":"2025-06-18T10:38:33.789202Z","steps":["trace[77173942] 'read index received'  (duration: 294.442755ms)","trace[77173942] 'applied index is now lower than readState.Index'  (duration: 19.851107ms)"],"step_count":2}
{"level":"warn","ts":"2025-06-18T10:38:33.870309Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-06-18T10:38:33.472713Z","time spent":"316.631203ms","remote":"127.0.0.1:49516","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":776,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/events/kube-system/kube-apiserver-minikube.184a1cd3aaa7b25a\" mod_revision:0 > success:<request_put:<key:\"/registry/events/kube-system/kube-apiserver-minikube.184a1cd3aaa7b25a\" value_size:689 lease:2289965522798569848 >> failure:<>"}
{"level":"info","ts":"2025-06-18T10:38:41.019767Z","caller":"traceutil/trace.go:171","msg":"trace[101298623] transaction","detail":"{read_only:false; response_revision:780; number_of_response:1; }","duration":"442.911793ms","start":"2025-06-18T10:38:40.576841Z","end":"2025-06-18T10:38:41.019753Z","steps":["trace[101298623] 'process raft request'  (duration: 442.462623ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-18T10:38:41.020255Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-06-18T10:38:40.576803Z","time spent":"443.128437ms","remote":"127.0.0.1:49632","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":4531,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/pods/kube-system/storage-provisioner\" mod_revision:766 > success:<request_put:<key:\"/registry/pods/kube-system/storage-provisioner\" value_size:4477 >> failure:<request_range:<key:\"/registry/pods/kube-system/storage-provisioner\" > >"}
{"level":"info","ts":"2025-06-18T10:38:41.038661Z","caller":"traceutil/trace.go:171","msg":"trace[1494265799] transaction","detail":"{read_only:false; response_revision:781; number_of_response:1; }","duration":"461.057592ms","start":"2025-06-18T10:38:40.577588Z","end":"2025-06-18T10:38:41.038646Z","steps":["trace[1494265799] 'process raft request'  (duration: 460.950974ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-18T10:38:41.038774Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-06-18T10:38:40.577571Z","time spent":"461.153526ms","remote":"127.0.0.1:49516","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":829,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/events/kube-system/storage-provisioner.184a1cd84b24ed6e\" mod_revision:0 > success:<request_put:<key:\"/registry/events/kube-system/storage-provisioner.184a1cd84b24ed6e\" value_size:746 lease:2289965522798569848 >> failure:<>"}
{"level":"info","ts":"2025-06-18T10:39:53.074665Z","caller":"traceutil/trace.go:171","msg":"trace[1336218631] transaction","detail":"{read_only:false; response_revision:840; number_of_response:1; }","duration":"109.229142ms","start":"2025-06-18T10:39:52.964275Z","end":"2025-06-18T10:39:53.073504Z","steps":["trace[1336218631] 'process raft request'  (duration: 108.853672ms)"],"step_count":1}
{"level":"info","ts":"2025-06-18T10:43:27.253929Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2025-06-18T10:43:27.258792Z","caller":"embed/etcd.go:408","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.67.2:2380"],"advertise-client-urls":["https://192.168.67.2:2379"]}
{"level":"info","ts":"2025-06-18T10:43:34.283187Z","caller":"etcdserver/server.go:1546","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"8688e899f7831fc7","current-leader-member-id":"8688e899f7831fc7"}
{"level":"warn","ts":"2025-06-18T10:43:34.286422Z","caller":"embed/serve.go:235","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-06-18T10:43:34.289971Z","caller":"embed/serve.go:237","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2025-06-18T10:43:34.289564Z","caller":"embed/serve.go:235","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.67.2:2379: use of closed network connection"}
{"level":"warn","ts":"2025-06-18T10:43:34.292835Z","caller":"embed/serve.go:237","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.67.2:2379: use of closed network connection"}
{"level":"info","ts":"2025-06-18T10:43:34.309180Z","caller":"embed/etcd.go:613","msg":"stopping serving peer traffic","address":"192.168.67.2:2380"}
{"level":"info","ts":"2025-06-18T10:43:34.311318Z","caller":"embed/etcd.go:618","msg":"stopped serving peer traffic","address":"192.168.67.2:2380"}
{"level":"info","ts":"2025-06-18T10:43:34.311374Z","caller":"embed/etcd.go:410","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.67.2:2380"],"advertise-client-urls":["https://192.168.67.2:2379"]}


==> etcd [4ecfa42c5eab] <==
{"level":"warn","ts":"2025-06-18T11:18:38.800400Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"170.991484ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 serializable:true keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-06-18T11:18:38.800530Z","caller":"traceutil/trace.go:171","msg":"trace[1521219904] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1445; }","duration":"171.133686ms","start":"2025-06-18T11:18:38.629377Z","end":"2025-06-18T11:18:38.800511Z","steps":["trace[1521219904] 'range keys from in-memory index tree'  (duration: 170.944537ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-18T11:18:38.952511Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"423.895524ms","expected-duration":"100ms","prefix":"","request":"header:<ID:2289965523329435516 > lease_revoke:<id:1fc79782bdf1472e>","response":"size:29"}
{"level":"info","ts":"2025-06-18T11:18:38.952935Z","caller":"traceutil/trace.go:171","msg":"trace[1932768369] transaction","detail":"{read_only:false; response_revision:1446; number_of_response:1; }","duration":"144.840462ms","start":"2025-06-18T11:18:38.808065Z","end":"2025-06-18T11:18:38.952906Z","steps":["trace[1932768369] 'process raft request'  (duration: 144.590589ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-18T11:22:02.930855Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"107.506419ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-06-18T11:22:02.931090Z","caller":"traceutil/trace.go:171","msg":"trace[821014543] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1640; }","duration":"135.657855ms","start":"2025-06-18T11:22:02.795367Z","end":"2025-06-18T11:22:02.931025Z","steps":["trace[821014543] 'range keys from in-memory index tree'  (duration: 107.403503ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-18T11:22:53.538814Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"233.95454ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/\" range_end:\"/registry/masterleases0\" ","response":"range_response_count:1 size:131"}
{"level":"info","ts":"2025-06-18T11:22:53.539059Z","caller":"traceutil/trace.go:171","msg":"trace[1785772179] range","detail":"{range_begin:/registry/masterleases/; range_end:/registry/masterleases0; response_count:1; response_revision:1705; }","duration":"234.154235ms","start":"2025-06-18T11:22:53.304798Z","end":"2025-06-18T11:22:53.538952Z","steps":["trace[1785772179] 'range keys from in-memory index tree'  (duration: 180.084559ms)"],"step_count":1}
{"level":"info","ts":"2025-06-18T11:22:59.328688Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1403}
{"level":"info","ts":"2025-06-18T11:22:59.512960Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":1403,"took":"178.836945ms","hash":1740017715,"current-db-size-bytes":3624960,"current-db-size":"3.6 MB","current-db-size-in-use-bytes":2404352,"current-db-size-in-use":"2.4 MB"}
{"level":"info","ts":"2025-06-18T11:22:59.513367Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":1740017715,"revision":1403,"compact-revision":-1}
{"level":"info","ts":"2025-06-18T11:23:10.374339Z","caller":"traceutil/trace.go:171","msg":"trace[367597332] transaction","detail":"{read_only:false; response_revision:1724; number_of_response:1; }","duration":"388.958814ms","start":"2025-06-18T11:23:09.985364Z","end":"2025-06-18T11:23:10.374323Z","steps":["trace[367597332] 'process raft request'  (duration: 388.671601ms)"],"step_count":1}
{"level":"info","ts":"2025-06-18T11:23:10.422805Z","caller":"traceutil/trace.go:171","msg":"trace[1969547870] linearizableReadLoop","detail":"{readStateIndex:2021; appliedIndex:2020; }","duration":"248.118747ms","start":"2025-06-18T11:23:10.126146Z","end":"2025-06-18T11:23:10.374265Z","steps":["trace[1969547870] 'read index received'  (duration: 247.828309ms)","trace[1969547870] 'applied index is now lower than readState.Index'  (duration: 289.905µs)"],"step_count":2}
{"level":"warn","ts":"2025-06-18T11:23:10.435991Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"248.416998ms","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-06-18T11:23:10.446382Z","caller":"traceutil/trace.go:171","msg":"trace[1464365259] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:1724; }","duration":"309.971336ms","start":"2025-06-18T11:23:10.126142Z","end":"2025-06-18T11:23:10.436114Z","steps":["trace[1464365259] 'agreement among raft nodes before linearized reading'  (duration: 248.399029ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-18T11:23:10.460448Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-06-18T11:23:09.985345Z","time spent":"389.158938ms","remote":"127.0.0.1:58996","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:1719 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2025-06-18T11:23:23.030053Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"205.769527ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-06-18T11:23:23.030138Z","caller":"traceutil/trace.go:171","msg":"trace[283059794] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1743; }","duration":"205.934755ms","start":"2025-06-18T11:23:22.824184Z","end":"2025-06-18T11:23:23.030119Z","steps":["trace[283059794] 'range keys from in-memory index tree'  (duration: 205.737361ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-18T11:23:23.030357Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"206.26921ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-06-18T11:23:23.030377Z","caller":"traceutil/trace.go:171","msg":"trace[1082326548] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:1743; }","duration":"206.311117ms","start":"2025-06-18T11:23:22.824059Z","end":"2025-06-18T11:23:23.030370Z","steps":["trace[1082326548] 'range keys from in-memory index tree'  (duration: 206.225241ms)"],"step_count":1}
{"level":"info","ts":"2025-06-18T11:27:59.332365Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":1711}
{"level":"info","ts":"2025-06-18T11:27:59.454500Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":1711,"took":"102.527749ms","hash":84755855,"current-db-size-bytes":3624960,"current-db-size":"3.6 MB","current-db-size-in-use-bytes":2433024,"current-db-size-in-use":"2.4 MB"}
{"level":"info","ts":"2025-06-18T11:27:59.455224Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":84755855,"revision":1711,"compact-revision":1403}
{"level":"warn","ts":"2025-06-18T11:29:23.541843Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"219.615066ms","expected-duration":"100ms","prefix":"","request":"header:<ID:2289965523329439024 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/replicasets/default/mongo-express-deployment-6965d87ff4\" mod_revision:2136 > success:<request_put:<key:\"/registry/replicasets/default/mongo-express-deployment-6965d87ff4\" value_size:2454 >> failure:<request_range:<key:\"/registry/replicasets/default/mongo-express-deployment-6965d87ff4\" > >>","response":"size:16"}
{"level":"warn","ts":"2025-06-18T11:29:23.609604Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.188295886s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/servicecidrs/kubernetes\" limit:1 ","response":"range_response_count:1 size:997"}
{"level":"info","ts":"2025-06-18T11:29:23.624514Z","caller":"traceutil/trace.go:171","msg":"trace[1265330779] transaction","detail":"{read_only:false; response_revision:2142; number_of_response:1; }","duration":"1.131098146s","start":"2025-06-18T11:29:22.472474Z","end":"2025-06-18T11:29:23.603572Z","steps":["trace[1265330779] 'process raft request'  (duration: 1.130723212s)"],"step_count":1}
{"level":"info","ts":"2025-06-18T11:29:23.624725Z","caller":"traceutil/trace.go:171","msg":"trace[56333579] transaction","detail":"{read_only:false; response_revision:2144; number_of_response:1; }","duration":"1.127182281s","start":"2025-06-18T11:29:22.476504Z","end":"2025-06-18T11:29:23.603686Z","steps":["trace[56333579] 'process raft request'  (duration: 1.126855753s)"],"step_count":1}
{"level":"info","ts":"2025-06-18T11:29:23.627611Z","caller":"traceutil/trace.go:171","msg":"trace[15342733] transaction","detail":"{read_only:false; response_revision:2140; number_of_response:1; }","duration":"1.190738262s","start":"2025-06-18T11:29:22.412387Z","end":"2025-06-18T11:29:23.603125Z","steps":["trace[15342733] 'process raft request'  (duration: 111.837897ms)","trace[15342733] 'compare'  (duration: 162.396585ms)"],"step_count":2}
{"level":"info","ts":"2025-06-18T11:29:23.627623Z","caller":"traceutil/trace.go:171","msg":"trace[796545790] transaction","detail":"{read_only:false; response_revision:2145; number_of_response:1; }","duration":"339.462551ms","start":"2025-06-18T11:29:23.264088Z","end":"2025-06-18T11:29:23.603550Z","steps":["trace[796545790] 'process raft request'  (duration: 339.361698ms)"],"step_count":1}
{"level":"info","ts":"2025-06-18T11:29:23.627586Z","caller":"traceutil/trace.go:171","msg":"trace[1431097711] range","detail":"{range_begin:/registry/servicecidrs/kubernetes; range_end:; response_count:1; response_revision:2145; }","duration":"1.193374169s","start":"2025-06-18T11:29:22.416351Z","end":"2025-06-18T11:29:23.609725Z","steps":["trace[1431097711] 'agreement among raft nodes before linearized reading'  (duration: 1.188244987s)"],"step_count":1}
{"level":"info","ts":"2025-06-18T11:29:23.624512Z","caller":"traceutil/trace.go:171","msg":"trace[1173941672] transaction","detail":"{read_only:false; response_revision:2141; number_of_response:1; }","duration":"1.134703423s","start":"2025-06-18T11:29:22.468906Z","end":"2025-06-18T11:29:23.603609Z","steps":["trace[1173941672] 'process raft request'  (duration: 1.13414673s)"],"step_count":1}
{"level":"info","ts":"2025-06-18T11:29:23.624697Z","caller":"traceutil/trace.go:171","msg":"trace[1586262028] transaction","detail":"{read_only:false; response_revision:2143; number_of_response:1; }","duration":"1.130461701s","start":"2025-06-18T11:29:22.473250Z","end":"2025-06-18T11:29:23.603712Z","steps":["trace[1586262028] 'process raft request'  (duration: 1.129985863s)"],"step_count":1}
{"level":"info","ts":"2025-06-18T11:29:23.627682Z","caller":"traceutil/trace.go:171","msg":"trace[1017458838] linearizableReadLoop","detail":"{readStateIndex:2519; appliedIndex:2518; }","duration":"1.187075174s","start":"2025-06-18T11:29:22.416377Z","end":"2025-06-18T11:29:23.603452Z","steps":["trace[1017458838] 'read index received'  (duration: 107.978351ms)","trace[1017458838] 'applied index is now lower than readState.Index'  (duration: 1.079093965s)"],"step_count":2}
{"level":"warn","ts":"2025-06-18T11:29:23.631922Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.168440518s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/pods/default/mongo-express-deployment-6965d87ff4-9sfml\" limit:1 ","response":"range_response_count:1 size:2346"}
{"level":"info","ts":"2025-06-18T11:29:23.632002Z","caller":"traceutil/trace.go:171","msg":"trace[482135710] range","detail":"{range_begin:/registry/pods/default/mongo-express-deployment-6965d87ff4-9sfml; range_end:; response_count:1; response_revision:2145; }","duration":"1.16849629s","start":"2025-06-18T11:29:22.463451Z","end":"2025-06-18T11:29:23.631947Z","steps":["trace[482135710] 'agreement among raft nodes before linearized reading'  (duration: 1.16840267s)"],"step_count":1}
{"level":"warn","ts":"2025-06-18T11:29:23.643140Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-06-18T11:29:22.472461Z","time spent":"1.155143717s","remote":"127.0.0.1:59112","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":933,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/endpointslices/default/mongo-express-service-gcb2z\" mod_revision:0 > success:<request_put:<key:\"/registry/endpointslices/default/mongo-express-service-gcb2z\" value_size:865 >> failure:<>"}
{"level":"warn","ts":"2025-06-18T11:29:23.639792Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-06-18T11:29:22.473240Z","time spent":"1.15465023s","remote":"127.0.0.1:59384","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":761,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/events/default/mongo-express-deployment-6965d87ff4-9sfml.184a1f9c8be3b4d6\" mod_revision:0 > success:<request_put:<key:\"/registry/events/default/mongo-express-deployment-6965d87ff4-9sfml.184a1f9c8be3b4d6\" value_size:660 lease:2289965523329438854 >> failure:<>"}
{"level":"warn","ts":"2025-06-18T11:29:23.667950Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-06-18T11:29:22.463439Z","time spent":"1.168629638s","remote":"127.0.0.1:59012","response type":"/etcdserverpb.KV/Range","request count":0,"request size":68,"response count":1,"response size":2370,"request content":"key:\"/registry/pods/default/mongo-express-deployment-6965d87ff4-9sfml\" limit:1 "}
{"level":"warn","ts":"2025-06-18T11:29:23.640179Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-06-18T11:29:22.468889Z","time spent":"1.158815313s","remote":"127.0.0.1:59304","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":3670,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/deployments/default/mongo-express-deployment\" mod_revision:2132 > success:<request_put:<key:\"/registry/deployments/default/mongo-express-deployment\" value_size:3608 >> failure:<request_range:<key:\"/registry/deployments/default/mongo-express-deployment\" > >"}
{"level":"warn","ts":"2025-06-18T11:29:23.639743Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-06-18T11:29:22.476492Z","time spent":"1.151112844s","remote":"127.0.0.1:58996","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":546,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/default/mongo-express-service\" mod_revision:0 > success:<request_put:<key:\"/registry/services/endpoints/default/mongo-express-service\" value_size:480 >> failure:<>"}
{"level":"warn","ts":"2025-06-18T11:29:23.643135Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-06-18T11:29:22.412372Z","time spent":"1.215296456s","remote":"127.0.0.1:59332","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":2527,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/replicasets/default/mongo-express-deployment-6965d87ff4\" mod_revision:2136 > success:<request_put:<key:\"/registry/replicasets/default/mongo-express-deployment-6965d87ff4\" value_size:2454 >> failure:<request_range:<key:\"/registry/replicasets/default/mongo-express-deployment-6965d87ff4\" > >"}
{"level":"warn","ts":"2025-06-18T11:29:23.639723Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-06-18T11:29:22.416334Z","time spent":"1.211328308s","remote":"127.0.0.1:59164","response type":"/etcdserverpb.KV/Range","request count":0,"request size":37,"response count":1,"response size":1021,"request content":"key:\"/registry/servicecidrs/kubernetes\" limit:1 "}
{"level":"warn","ts":"2025-06-18T11:29:23.639741Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-06-18T11:29:23.264070Z","time spent":"363.576198ms","remote":"127.0.0.1:59098","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":672,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:2118 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:599 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"warn","ts":"2025-06-18T11:29:24.173149Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"960.795453ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/masterleases/192.168.67.2\" limit:1 ","response":"range_response_count:1 size:131"}
{"level":"info","ts":"2025-06-18T11:29:24.173258Z","caller":"traceutil/trace.go:171","msg":"trace[1674424427] range","detail":"{range_begin:/registry/masterleases/192.168.67.2; range_end:; response_count:1; response_revision:2145; }","duration":"960.978776ms","start":"2025-06-18T11:29:23.212266Z","end":"2025-06-18T11:29:24.173245Z","steps":["trace[1674424427] 'agreement among raft nodes before linearized reading'  (duration: 458.754794ms)","trace[1674424427] 'range keys from in-memory index tree'  (duration: 502.04225ms)"],"step_count":2}
{"level":"warn","ts":"2025-06-18T11:29:24.173290Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-06-18T11:29:23.212248Z","time spent":"961.034049ms","remote":"127.0.0.1:58856","response type":"/etcdserverpb.KV/Range","request count":0,"request size":39,"response count":1,"response size":155,"request content":"key:\"/registry/masterleases/192.168.67.2\" limit:1 "}
{"level":"warn","ts":"2025-06-18T11:29:24.173440Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"578.633922ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/\" range_end:\"/registry/services/endpoints0\" count_only:true ","response":"range_response_count:0 size:7"}
{"level":"warn","ts":"2025-06-18T11:29:24.174485Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.587465235s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/serviceaccounts/default/default\" limit:1 ","response":"range_response_count:1 size:171"}
{"level":"info","ts":"2025-06-18T11:29:24.174967Z","caller":"traceutil/trace.go:171","msg":"trace[7603614] range","detail":"{range_begin:/registry/serviceaccounts/default/default; range_end:; response_count:1; response_revision:2145; }","duration":"1.587994353s","start":"2025-06-18T11:29:22.586961Z","end":"2025-06-18T11:29:24.174956Z","steps":["trace[7603614] 'agreement among raft nodes before linearized reading'  (duration: 1.084074001s)","trace[7603614] 'range keys from in-memory index tree'  (duration: 500.732693ms)"],"step_count":2}
{"level":"warn","ts":"2025-06-18T11:29:24.175021Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-06-18T11:29:22.586935Z","time spent":"1.58806612s","remote":"127.0.0.1:59032","response type":"/etcdserverpb.KV/Range","request count":0,"request size":45,"response count":1,"response size":195,"request content":"key:\"/registry/serviceaccounts/default/default\" limit:1 "}
{"level":"warn","ts":"2025-06-18T11:29:24.174505Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.071068489s","expected-duration":"100ms","prefix":"read-only range ","request":"limit:1 keys_only:true ","response":"range_response_count:0 size:5"}
{"level":"info","ts":"2025-06-18T11:29:24.175141Z","caller":"traceutil/trace.go:171","msg":"trace[630174564] range","detail":"{range_begin:; range_end:; response_count:0; response_revision:2145; }","duration":"1.071732203s","start":"2025-06-18T11:29:23.103401Z","end":"2025-06-18T11:29:24.175133Z","steps":["trace[630174564] 'agreement among raft nodes before linearized reading'  (duration: 567.628799ms)","trace[630174564] 'range keys from in-memory index tree'  (duration: 503.430109ms)"],"step_count":2}
{"level":"info","ts":"2025-06-18T11:29:24.174719Z","caller":"traceutil/trace.go:171","msg":"trace[1941628328] transaction","detail":"{read_only:false; response_revision:2146; number_of_response:1; }","duration":"407.315375ms","start":"2025-06-18T11:29:23.767389Z","end":"2025-06-18T11:29:24.174704Z","steps":["trace[1941628328] 'process raft request'  (duration: 404.243712ms)"],"step_count":1}
{"level":"warn","ts":"2025-06-18T11:29:24.175323Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-06-18T11:29:23.767366Z","time spent":"407.915782ms","remote":"127.0.0.1:59012","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":3509,"response count":0,"response size":40,"request content":"compare:<target:MOD key:\"/registry/pods/default/mongo-express-deployment-6965d87ff4-9sfml\" mod_revision:2135 > success:<request_put:<key:\"/registry/pods/default/mongo-express-deployment-6965d87ff4-9sfml\" value_size:3437 >> failure:<request_range:<key:\"/registry/pods/default/mongo-express-deployment-6965d87ff4-9sfml\" > >"}
{"level":"info","ts":"2025-06-18T11:29:24.174905Z","caller":"traceutil/trace.go:171","msg":"trace[1233073988] range","detail":"{range_begin:/registry/services/endpoints/; range_end:/registry/services/endpoints0; response_count:0; response_revision:2145; }","duration":"578.692824ms","start":"2025-06-18T11:29:23.594780Z","end":"2025-06-18T11:29:24.173472Z","steps":["trace[1233073988] 'agreement among raft nodes before linearized reading'  (duration: 76.208228ms)","trace[1233073988] 'count revisions from in-memory index tree'  (duration: 502.414248ms)"],"step_count":2}
{"level":"warn","ts":"2025-06-18T11:29:24.175394Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2025-06-18T11:29:23.594769Z","time spent":"580.619948ms","remote":"127.0.0.1:58996","response type":"/etcdserverpb.KV/Range","request count":0,"request size":64,"response count":5,"response size":31,"request content":"key:\"/registry/services/endpoints/\" range_end:\"/registry/services/endpoints0\" count_only:true "}
{"level":"info","ts":"2025-06-18T11:32:59.341317Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":2028}
{"level":"info","ts":"2025-06-18T11:32:59.391968Z","caller":"mvcc/kvstore_compaction.go:71","msg":"finished scheduled compaction","compact-revision":2028,"took":"45.043443ms","hash":3630003141,"current-db-size-bytes":3624960,"current-db-size":"3.6 MB","current-db-size-in-use-bytes":2342912,"current-db-size-in-use":"2.3 MB"}
{"level":"info","ts":"2025-06-18T11:32:59.393749Z","caller":"mvcc/hash.go:151","msg":"storing new hash","hash":3630003141,"revision":2028,"compact-revision":1711}
{"level":"info","ts":"2025-06-18T11:33:25.957601Z","caller":"traceutil/trace.go:171","msg":"trace[1428495695] transaction","detail":"{read_only:false; response_revision:2384; number_of_response:1; }","duration":"115.828841ms","start":"2025-06-18T11:33:25.841144Z","end":"2025-06-18T11:33:25.956973Z","steps":["trace[1428495695] 'process raft request'  (duration: 115.619098ms)"],"step_count":1}


==> kernel <==
 11:34:34 up  2:33,  0 users,  load average: 0.79, 0.87, 1.53
Linux minikube 5.15.167.4-microsoft-standard-WSL2 #1 SMP Tue Nov 5 00:21:55 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.5 LTS"


==> kube-apiserver [110afad83422] <==
I0618 11:13:01.224988       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0618 11:13:01.226924       1 controller.go:119] Starting legacy_token_tracking_controller
I0618 11:13:01.226991       1 shared_informer.go:350] "Waiting for caches to sync" controller="configmaps"
I0618 11:13:01.228317       1 dynamic_serving_content.go:135] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0618 11:13:01.218483       1 system_namespaces_controller.go:66] Starting system namespaces controller
I0618 11:13:01.228506       1 crdregistration_controller.go:114] Starting crd-autoregister controller
I0618 11:13:01.228514       1 shared_informer.go:350] "Waiting for caches to sync" controller="crd-autoregister"
I0618 11:13:01.228658       1 controller.go:142] Starting OpenAPI controller
I0618 11:13:01.228814       1 controller.go:90] Starting OpenAPI V3 controller
I0618 11:13:01.228967       1 naming_controller.go:299] Starting NamingConditionController
I0618 11:13:01.229054       1 establishing_controller.go:81] Starting EstablishingController
I0618 11:13:01.229611       1 nonstructuralschema_controller.go:195] Starting NonStructuralSchemaConditionController
I0618 11:13:01.229669       1 apiapproval_controller.go:189] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0618 11:13:01.230075       1 crd_finalizer.go:269] Starting CRDFinalizer
I0618 11:13:01.230129       1 dynamic_cafile_content.go:161] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0618 11:13:01.230194       1 dynamic_cafile_content.go:161] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0618 11:13:01.232000       1 default_servicecidr_controller.go:110] Starting kubernetes-service-cidr-controller
I0618 11:13:01.232035       1 shared_informer.go:350] "Waiting for caches to sync" controller="kubernetes-service-cidr-controller"
I0618 11:13:01.433153       1 shared_informer.go:357] "Caches are synced" controller="kubernetes-service-cidr-controller"
I0618 11:13:01.433812       1 default_servicecidr_controller.go:136] Shutting down kubernetes-service-cidr-controller
I0618 11:13:01.517685       1 apf_controller.go:382] Running API Priority and Fairness config worker
I0618 11:13:01.518182       1 apf_controller.go:385] Running API Priority and Fairness periodic rebalancing process
I0618 11:13:01.518450       1 cidrallocator.go:301] created ClusterIP allocator for Service CIDR 10.96.0.0/12
I0618 11:13:01.518718       1 shared_informer.go:357] "Caches are synced" controller="cluster_authentication_trust_controller"
I0618 11:13:01.519036       1 shared_informer.go:357] "Caches are synced" controller="*generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]"
I0618 11:13:01.519870       1 policy_source.go:240] refreshing policies
I0618 11:13:01.522685       1 shared_informer.go:357] "Caches are synced" controller="crd-autoregister"
I0618 11:13:01.522803       1 aggregator.go:171] initial CRD sync complete...
I0618 11:13:01.524793       1 controller.go:667] quota admission added evaluator for: leases.coordination.k8s.io
I0618 11:13:01.525708       1 autoregister_controller.go:144] Starting autoregister controller
I0618 11:13:01.525747       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0618 11:13:01.525788       1 cache.go:39] Caches are synced for autoregister controller
I0618 11:13:01.527339       1 shared_informer.go:357] "Caches are synced" controller="configmaps"
I0618 11:13:01.527882       1 cache.go:39] Caches are synced for LocalAvailability controller
I0618 11:13:01.615764       1 shared_informer.go:357] "Caches are synced" controller="node_authorizer"
I0618 11:13:01.618004       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0618 11:13:01.618662       1 cache.go:39] Caches are synced for RemoteAvailability controller
I0618 11:13:01.621830       1 shared_informer.go:357] "Caches are synced" controller="ipallocator-repair-controller"
I0618 11:13:01.633747       1 handler_discovery.go:451] Starting ResourceDiscoveryManager
I0618 11:13:02.222591       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0618 11:13:03.569185       1 controller.go:667] quota admission added evaluator for: serviceaccounts
I0618 11:13:04.995758       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0618 11:13:06.363846       1 controller.go:667] quota admission added evaluator for: daemonsets.apps
I0618 11:13:06.391631       1 controller.go:667] quota admission added evaluator for: endpoints
I0618 11:13:06.391696       1 controller.go:667] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0618 11:13:06.392418       1 controller.go:667] quota admission added evaluator for: replicasets.apps
I0618 11:13:06.434725       1 controller.go:667] quota admission added evaluator for: deployments.apps
I0618 11:16:48.559573       1 alloc.go:328] "allocated clusterIPs" service="default/mongodb-service" clusterIPs={"IPv4":"10.111.203.199"}
I0618 11:16:48.618088       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0618 11:22:41.913209       1 alloc.go:328] "allocated clusterIPs" service="default/mongo-express-service" clusterIPs={"IPv4":"10.104.115.89"}
I0618 11:22:42.014348       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0618 11:22:42.116064       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0618 11:23:01.395414       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0618 11:26:01.288491       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0618 11:26:28.439369       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0618 11:28:54.969625       1 alloc.go:328] "allocated clusterIPs" service="default/mongodb-service" clusterIPs={"IPv4":"10.102.54.94"}
I0618 11:28:54.971197       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0618 11:29:22.405953       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12
I0618 11:29:22.414895       1 alloc.go:328] "allocated clusterIPs" service="default/mongo-express-service" clusterIPs={"IPv4":"10.96.137.42"}
I0618 11:33:01.358581       1 cidrallocator.go:277] updated ClusterIP allocator for Service CIDR 10.96.0.0/12


==> kube-apiserver [a6344c6c6e60] <==
W0618 10:43:32.976253       1 logging.go:55] [core] [Channel #181 SubChannel #182]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:32.999089       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:33.011645       1 logging.go:55] [core] [Channel #82 SubChannel #83]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:33.050556       1 logging.go:55] [core] [Channel #79 SubChannel #80]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:33.081322       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:33.082545       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:33.108624       1 logging.go:55] [core] [Channel #7 SubChannel #8]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:35.312759       1 logging.go:55] [core] [Channel #142 SubChannel #143]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:35.716707       1 logging.go:55] [core] [Channel #166 SubChannel #167]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:35.790087       1 logging.go:55] [core] [Channel #157 SubChannel #158]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:35.973494       1 logging.go:55] [core] [Channel #25 SubChannel #26]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.014438       1 logging.go:55] [core] [Channel #97 SubChannel #98]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.027298       1 logging.go:55] [core] [Channel #46 SubChannel #47]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.107282       1 logging.go:55] [core] [Channel #100 SubChannel #101]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.126216       1 logging.go:55] [core] [Channel #73 SubChannel #74]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.155191       1 logging.go:55] [core] [Channel #52 SubChannel #53]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.166615       1 logging.go:55] [core] [Channel #154 SubChannel #155]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.189025       1 logging.go:55] [core] [Channel #184 SubChannel #185]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.243288       1 logging.go:55] [core] [Channel #91 SubChannel #92]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.254152       1 logging.go:55] [core] [Channel #40 SubChannel #41]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.270194       1 logging.go:55] [core] [Channel #28 SubChannel #29]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.278122       1 logging.go:55] [core] [Channel #148 SubChannel #149]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.281950       1 logging.go:55] [core] [Channel #139 SubChannel #140]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.321619       1 logging.go:55] [core] [Channel #2 SubChannel #3]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.350622       1 logging.go:55] [core] [Channel #64 SubChannel #65]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.355400       1 logging.go:55] [core] [Channel #118 SubChannel #119]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.378970       1 logging.go:55] [core] [Channel #1 SubChannel #4]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.408460       1 logging.go:55] [core] [Channel #31 SubChannel #32]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.418300       1 logging.go:55] [core] [Channel #10 SubChannel #11]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.425222       1 logging.go:55] [core] [Channel #88 SubChannel #89]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.431300       1 logging.go:55] [core] [Channel #22 SubChannel #23]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.469335       1 logging.go:55] [core] [Channel #160 SubChannel #161]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.472155       1 logging.go:55] [core] [Channel #37 SubChannel #38]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.472724       1 logging.go:55] [core] [Channel #133 SubChannel #134]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.515345       1 logging.go:55] [core] [Channel #109 SubChannel #110]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.540326       1 logging.go:55] [core] [Channel #34 SubChannel #35]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.579989       1 logging.go:55] [core] [Channel #115 SubChannel #116]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.599858       1 logging.go:55] [core] [Channel #85 SubChannel #86]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.608052       1 logging.go:55] [core] [Channel #175 SubChannel #176]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.644264       1 logging.go:55] [core] [Channel #67 SubChannel #68]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.657842       1 logging.go:55] [core] [Channel #163 SubChannel #164]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.704900       1 logging.go:55] [core] [Channel #172 SubChannel #173]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.705161       1 logging.go:55] [core] [Channel #121 SubChannel #122]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.763060       1 logging.go:55] [core] [Channel #124 SubChannel #125]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.795094       1 logging.go:55] [core] [Channel #130 SubChannel #131]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.804707       1 logging.go:55] [core] [Channel #112 SubChannel #113]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.846532       1 logging.go:55] [core] [Channel #70 SubChannel #71]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.847218       1 logging.go:55] [core] [Channel #103 SubChannel #104]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.859150       1 logging.go:55] [core] [Channel #178 SubChannel #179]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.861911       1 logging.go:55] [core] [Channel #106 SubChannel #107]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:36.938205       1 logging.go:55] [core] [Channel #61 SubChannel #62]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:37.042390       1 logging.go:55] [core] [Channel #55 SubChannel #56]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:37.104750       1 logging.go:55] [core] [Channel #94 SubChannel #95]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:37.225522       1 logging.go:55] [core] [Channel #169 SubChannel #170]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:37.274929       1 logging.go:55] [core] [Channel #58 SubChannel #59]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:37.354560       1 logging.go:55] [core] [Channel #136 SubChannel #137]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:37.377518       1 logging.go:55] [core] [Channel #181 SubChannel #182]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:37.396071       1 logging.go:55] [core] [Channel #145 SubChannel #146]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:37.411050       1 logging.go:55] [core] [Channel #76 SubChannel #77]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0618 10:43:37.449511       1 logging.go:55] [core] [Channel #19 SubChannel #20]grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-controller-manager [2cdb01faa492] <==
I0618 10:38:37.446845       1 resource_quota_monitor.go:227] "QuotaMonitor created object count evaluator" logger="resourcequota-controller" resource="poddisruptionbudgets.policy"
I0618 10:38:37.446907       1 controllermanager.go:778] "Started controller" controller="resourcequota-controller"
I0618 10:38:37.447029       1 resource_quota_controller.go:300] "Starting resource quota controller" logger="resourcequota-controller"
I0618 10:38:37.447051       1 shared_informer.go:350] "Waiting for caches to sync" controller="resource quota"
I0618 10:38:37.447097       1 resource_quota_monitor.go:308] "QuotaMonitor running" logger="resourcequota-controller"
I0618 10:38:37.489061       1 controllermanager.go:778] "Started controller" controller="certificatesigningrequest-cleaner-controller"
I0618 10:38:37.489120       1 cleaner.go:83] "Starting CSR cleaner controller" logger="certificatesigningrequest-cleaner-controller"
I0618 10:38:37.584579       1 shared_informer.go:350] "Waiting for caches to sync" controller="resource quota"
I0618 10:38:37.610259       1 shared_informer.go:357] "Caches are synced" controller="expand"
I0618 10:38:37.616346       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0618 10:38:37.622866       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrapproving"
I0618 10:38:37.632469       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0618 10:38:37.632577       1 shared_informer.go:357] "Caches are synced" controller="namespace"
I0618 10:38:37.632761       1 shared_informer.go:357] "Caches are synced" controller="service-cidr-controller"
I0618 10:38:37.633731       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0618 10:38:37.633868       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0618 10:38:37.637106       1 shared_informer.go:357] "Caches are synced" controller="PV protection"
I0618 10:38:37.669536       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice_mirroring"
I0618 10:38:37.670995       1 shared_informer.go:357] "Caches are synced" controller="crt configmap"
I0618 10:38:37.676178       1 shared_informer.go:357] "Caches are synced" controller="deployment"
I0618 10:38:37.679276       1 shared_informer.go:357] "Caches are synced" controller="ReplicaSet"
I0618 10:38:37.681064       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0618 10:38:37.683030       1 shared_informer.go:357] "Caches are synced" controller="service account"
I0618 10:38:37.684297       1 shared_informer.go:357] "Caches are synced" controller="ReplicationController"
I0618 10:38:37.691230       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0618 10:38:37.692965       1 shared_informer.go:357] "Caches are synced" controller="ephemeral"
I0618 10:38:37.693054       1 shared_informer.go:357] "Caches are synced" controller="bootstrap_signer"
I0618 10:38:37.693793       1 shared_informer.go:357] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0618 10:38:37.696768       1 shared_informer.go:357] "Caches are synced" controller="stateful set"
I0618 10:38:37.706552       1 shared_informer.go:357] "Caches are synced" controller="HPA"
I0618 10:38:37.769562       1 shared_informer.go:357] "Caches are synced" controller="cronjob"
I0618 10:38:37.769597       1 shared_informer.go:357] "Caches are synced" controller="persistent volume"
I0618 10:38:37.769839       1 shared_informer.go:357] "Caches are synced" controller="TTL after finished"
I0618 10:38:37.769996       1 shared_informer.go:357] "Caches are synced" controller="daemon sets"
I0618 10:38:37.770431       1 shared_informer.go:357] "Caches are synced" controller="taint"
I0618 10:38:37.770690       1 shared_informer.go:357] "Caches are synced" controller="node"
I0618 10:38:37.770787       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0618 10:38:37.770848       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0618 10:38:37.770875       1 shared_informer.go:350] "Waiting for caches to sync" controller="cidrallocator"
I0618 10:38:37.770900       1 shared_informer.go:357] "Caches are synced" controller="cidrallocator"
I0618 10:38:37.771374       1 shared_informer.go:357] "Caches are synced" controller="ClusterRoleAggregator"
I0618 10:38:37.772154       1 shared_informer.go:357] "Caches are synced" controller="PVC protection"
I0618 10:38:37.772797       1 shared_informer.go:357] "Caches are synced" controller="endpoint"
I0618 10:38:37.773882       1 shared_informer.go:357] "Caches are synced" controller="GC"
I0618 10:38:37.774450       1 shared_informer.go:357] "Caches are synced" controller="TTL"
I0618 10:38:37.775911       1 shared_informer.go:357] "Caches are synced" controller="taint-eviction-controller"
I0618 10:38:37.777918       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0618 10:38:37.784205       1 shared_informer.go:357] "Caches are synced" controller="job"
I0618 10:38:37.783891       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0618 10:38:37.785079       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice"
I0618 10:38:37.784417       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0618 10:38:37.827100       1 shared_informer.go:357] "Caches are synced" controller="attach detach"
I0618 10:38:37.842432       1 shared_informer.go:357] "Caches are synced" controller="disruption"
I0618 10:38:37.848164       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0618 10:38:37.885697       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0618 10:38:37.943955       1 shared_informer.go:357] "Caches are synced" controller="validatingadmissionpolicy-status"
I0618 10:38:38.383607       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0618 10:38:38.383668       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0618 10:38:38.383675       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0618 10:38:38.392623       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"


==> kube-controller-manager [a5a7832dce37] <==
I0618 11:13:04.726230       1 controllermanager.go:778] "Started controller" controller="resourcequota-controller"
I0618 11:13:04.726318       1 resource_quota_controller.go:300] "Starting resource quota controller" logger="resourcequota-controller"
I0618 11:13:04.726373       1 shared_informer.go:350] "Waiting for caches to sync" controller="resource quota"
I0618 11:13:04.726417       1 resource_quota_monitor.go:308] "QuotaMonitor running" logger="resourcequota-controller"
I0618 11:13:04.760766       1 controllermanager.go:778] "Started controller" controller="replicaset-controller"
I0618 11:13:04.760989       1 replica_set.go:219] "Starting controller" logger="replicaset-controller" name="replicaset"
I0618 11:13:04.761018       1 shared_informer.go:350] "Waiting for caches to sync" controller="ReplicaSet"
I0618 11:13:04.766078       1 shared_informer.go:350] "Waiting for caches to sync" controller="resource quota"
I0618 11:13:04.784240       1 actual_state_of_world.go:541] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0618 11:13:04.818110       1 shared_informer.go:357] "Caches are synced" controller="validatingadmissionpolicy-status"
I0618 11:13:04.818220       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-client"
I0618 11:13:04.818420       1 shared_informer.go:357] "Caches are synced" controller="expand"
I0618 11:13:04.818770       1 shared_informer.go:357] "Caches are synced" controller="endpoint"
I0618 11:13:04.818162       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kube-apiserver-client"
I0618 11:13:04.819356       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-kubelet-serving"
I0618 11:13:04.820173       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrsigning-legacy-unknown"
I0618 11:13:04.819723       1 shared_informer.go:357] "Caches are synced" controller="HPA"
I0618 11:13:04.818842       1 shared_informer.go:357] "Caches are synced" controller="legacy-service-account-token-cleaner"
I0618 11:13:04.819774       1 shared_informer.go:357] "Caches are synced" controller="service account"
I0618 11:13:04.818153       1 shared_informer.go:357] "Caches are synced" controller="taint-eviction-controller"
I0618 11:13:04.821780       1 shared_informer.go:357] "Caches are synced" controller="node"
I0618 11:13:04.820030       1 shared_informer.go:357] "Caches are synced" controller="ReplicationController"
I0618 11:13:04.823033       1 range_allocator.go:177] "Sending events to api server" logger="node-ipam-controller"
I0618 11:13:04.823431       1 range_allocator.go:183] "Starting range CIDR allocator" logger="node-ipam-controller"
I0618 11:13:04.823473       1 shared_informer.go:350] "Waiting for caches to sync" controller="cidrallocator"
I0618 11:13:04.823496       1 shared_informer.go:357] "Caches are synced" controller="cidrallocator"
I0618 11:13:04.824362       1 shared_informer.go:350] "Waiting for caches to sync" controller="garbage collector"
I0618 11:13:04.825355       1 shared_informer.go:357] "Caches are synced" controller="daemon sets"
I0618 11:13:04.826060       1 shared_informer.go:357] "Caches are synced" controller="TTL"
I0618 11:13:04.835128       1 shared_informer.go:357] "Caches are synced" controller="namespace"
I0618 11:13:04.835473       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice_mirroring"
I0618 11:13:04.839436       1 shared_informer.go:357] "Caches are synced" controller="deployment"
I0618 11:13:04.840950       1 shared_informer.go:357] "Caches are synced" controller="GC"
I0618 11:13:04.844805       1 shared_informer.go:357] "Caches are synced" controller="ClusterRoleAggregator"
I0618 11:13:04.851882       1 shared_informer.go:357] "Caches are synced" controller="taint"
I0618 11:13:04.853455       1 node_lifecycle_controller.go:1221] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0618 11:13:04.855615       1 node_lifecycle_controller.go:873] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0618 11:13:04.857115       1 node_lifecycle_controller.go:1067] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0618 11:13:04.861860       1 shared_informer.go:357] "Caches are synced" controller="ReplicaSet"
I0618 11:13:04.862143       1 shared_informer.go:357] "Caches are synced" controller="ephemeral"
I0618 11:13:04.915958       1 shared_informer.go:357] "Caches are synced" controller="persistent volume"
I0618 11:13:04.916033       1 shared_informer.go:357] "Caches are synced" controller="certificate-csrapproving"
I0618 11:13:04.916222       1 shared_informer.go:357] "Caches are synced" controller="endpoint_slice"
I0618 11:13:04.916405       1 shared_informer.go:357] "Caches are synced" controller="attach detach"
I0618 11:13:04.916474       1 shared_informer.go:357] "Caches are synced" controller="stateful set"
I0618 11:13:04.916843       1 shared_informer.go:357] "Caches are synced" controller="PVC protection"
I0618 11:13:04.917116       1 shared_informer.go:357] "Caches are synced" controller="PV protection"
I0618 11:13:04.932189       1 shared_informer.go:357] "Caches are synced" controller="service-cidr-controller"
I0618 11:13:05.022971       1 shared_informer.go:357] "Caches are synced" controller="disruption"
I0618 11:13:05.082475       1 shared_informer.go:357] "Caches are synced" controller="job"
I0618 11:13:05.084918       1 shared_informer.go:357] "Caches are synced" controller="cronjob"
I0618 11:13:05.113693       1 shared_informer.go:357] "Caches are synced" controller="TTL after finished"
I0618 11:13:05.126738       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0618 11:13:05.135911       1 shared_informer.go:357] "Caches are synced" controller="bootstrap_signer"
I0618 11:13:05.166369       1 shared_informer.go:357] "Caches are synced" controller="resource quota"
I0618 11:13:05.188213       1 shared_informer.go:357] "Caches are synced" controller="crt configmap"
I0618 11:13:05.589887       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"
I0618 11:13:05.589950       1 garbagecollector.go:154] "Garbage collector: all resource monitors have synced" logger="garbage-collector-controller"
I0618 11:13:05.589959       1 garbagecollector.go:157] "Proceeding to collect garbage" logger="garbage-collector-controller"
I0618 11:13:05.625614       1 shared_informer.go:357] "Caches are synced" controller="garbage collector"


==> kube-proxy [7c775dc58144] <==
I0618 11:13:16.273064       1 server_linux.go:63] "Using iptables proxy"
I0618 11:13:16.690734       1 server.go:715] "Successfully retrieved node IP(s)" IPs=["192.168.67.2"]
E0618 11:13:16.691104       1 server.go:245] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0618 11:13:16.728764       1 server.go:254] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0618 11:13:16.728919       1 server_linux.go:145] "Using iptables Proxier"
I0618 11:13:16.734340       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0618 11:13:16.740979       1 proxier.go:271] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0618 11:13:16.748064       1 proxier.go:271] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0618 11:13:16.748497       1 server.go:516] "Version info" version="v1.33.1"
I0618 11:13:16.748539       1 server.go:518] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0618 11:13:16.756858       1 metrics.go:379] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0618 11:13:16.763090       1 metrics.go:379] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0618 11:13:16.766760       1 config.go:199] "Starting service config controller"
I0618 11:13:16.766901       1 config.go:105] "Starting endpoint slice config controller"
I0618 11:13:16.767398       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0618 11:13:16.767399       1 shared_informer.go:350] "Waiting for caches to sync" controller="service config"
I0618 11:13:16.767666       1 config.go:329] "Starting node config controller"
I0618 11:13:16.767682       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0618 11:13:16.767723       1 config.go:440] "Starting serviceCIDR config controller"
I0618 11:13:16.767732       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0618 11:13:16.867951       1 shared_informer.go:357] "Caches are synced" controller="node config"
I0618 11:13:16.868088       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"
I0618 11:13:16.868413       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"
I0618 11:13:16.868458       1 shared_informer.go:357] "Caches are synced" controller="service config"


==> kube-proxy [b34889263aec] <==
I0618 10:38:39.759358       1 server_linux.go:63] "Using iptables proxy"
I0618 10:38:41.488779       1 server.go:715] "Successfully retrieved node IP(s)" IPs=["192.168.67.2"]
E0618 10:38:41.489410       1 server.go:245] "Kube-proxy configuration may be incomplete or incorrect" err="nodePortAddresses is unset; NodePort connections will be accepted on all local IPs. Consider using `--nodeport-addresses primary`"
I0618 10:38:41.526267       1 server.go:254] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0618 10:38:41.526361       1 server_linux.go:145] "Using iptables Proxier"
I0618 10:38:41.532935       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses" ipFamily="IPv4"
E0618 10:38:41.539608       1 proxier.go:271] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv4"
E0618 10:38:41.547066       1 proxier.go:271] "Failed to create nfacct runner, nfacct based metrics won't be available" err="nfacct sub-system not available" ipFamily="IPv6"
I0618 10:38:41.549254       1 server.go:516] "Version info" version="v1.33.1"
I0618 10:38:41.549291       1 server.go:518] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
E0618 10:38:41.555562       1 metrics.go:379] "failed to initialize nfacct client" err="nfacct sub-system not available"
E0618 10:38:41.561279       1 metrics.go:379] "failed to initialize nfacct client" err="nfacct sub-system not available"
I0618 10:38:41.564242       1 config.go:199] "Starting service config controller"
I0618 10:38:41.564390       1 config.go:105] "Starting endpoint slice config controller"
I0618 10:38:41.564823       1 config.go:440] "Starting serviceCIDR config controller"
I0618 10:38:41.565452       1 config.go:329] "Starting node config controller"
I0618 10:38:41.566135       1 shared_informer.go:350] "Waiting for caches to sync" controller="serviceCIDR config"
I0618 10:38:41.566153       1 shared_informer.go:350] "Waiting for caches to sync" controller="node config"
I0618 10:38:41.566167       1 shared_informer.go:350] "Waiting for caches to sync" controller="service config"
I0618 10:38:41.566193       1 shared_informer.go:350] "Waiting for caches to sync" controller="endpoint slice config"
I0618 10:38:41.666361       1 shared_informer.go:357] "Caches are synced" controller="serviceCIDR config"
I0618 10:38:41.666571       1 shared_informer.go:357] "Caches are synced" controller="node config"
I0618 10:38:41.666732       1 shared_informer.go:357] "Caches are synced" controller="service config"
I0618 10:38:41.667072       1 shared_informer.go:357] "Caches are synced" controller="endpoint slice config"


==> kube-scheduler [48a9dbeb83b9] <==
I0618 11:12:57.637201       1 serving.go:386] Generated self-signed cert in-memory
W0618 11:13:01.437243       1 requestheader_controller.go:204] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0618 11:13:01.437464       1 authentication.go:397] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system": RBAC: [clusterrole.rbac.authorization.k8s.io "system:basic-user" not found, clusterrole.rbac.authorization.k8s.io "system:discovery" not found, clusterrole.rbac.authorization.k8s.io "system:volume-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:kube-scheduler" not found, clusterrole.rbac.authorization.k8s.io "system:public-info-viewer" not found, role.rbac.authorization.k8s.io "system::leader-locking-kube-scheduler" not found, role.rbac.authorization.k8s.io "extension-apiserver-authentication-reader" not found]
W0618 11:13:01.437498       1 authentication.go:398] Continuing without authentication configuration. This may treat all requests as anonymous.
W0618 11:13:01.437661       1 authentication.go:399] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0618 11:13:01.732269       1 server.go:171] "Starting Kubernetes Scheduler" version="v1.33.1"
I0618 11:13:01.732468       1 server.go:173] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0618 11:13:01.823330       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0618 11:13:01.825912       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0618 11:13:01.830134       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0618 11:13:01.830639       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0618 11:13:01.931994       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"


==> kube-scheduler [50a8afab30f2] <==
I0618 10:38:24.693328       1 serving.go:386] Generated self-signed cert in-memory
I0618 10:38:31.521498       1 server.go:171] "Starting Kubernetes Scheduler" version="v1.33.1"
I0618 10:38:31.521697       1 server.go:173] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0618 10:38:31.596490       1 secure_serving.go:211] Serving securely on 127.0.0.1:10259
I0618 10:38:31.598470       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0618 10:38:31.598642       1 configmap_cafile_content.go:205] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0618 10:38:31.599610       1 requestheader_controller.go:180] Starting RequestHeaderAuthRequestController
I0618 10:38:31.600058       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0618 10:38:31.600715       1 shared_informer.go:350] "Waiting for caches to sync" controller="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0618 10:38:31.600743       1 shared_informer.go:350] "Waiting for caches to sync" controller="RequestHeaderAuthRequestController"
I0618 10:38:31.614673       1 tlsconfig.go:243] "Starting DynamicServingCertificateController"
I0618 10:38:31.701162       1 shared_informer.go:357] "Caches are synced" controller="RequestHeaderAuthRequestController"
I0618 10:38:31.701162       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0618 10:38:31.701482       1 shared_informer.go:357] "Caches are synced" controller="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0618 10:43:27.356144       1 configmap_cafile_content.go:226] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0618 10:43:27.358100       1 requestheader_controller.go:194] Shutting down RequestHeaderAuthRequestController
I0618 10:43:27.359652       1 tlsconfig.go:258] "Shutting down DynamicServingCertificateController"
I0618 10:43:27.361159       1 secure_serving.go:259] Stopped listening on 127.0.0.1:10259
I0618 10:43:27.356167       1 configmap_cafile_content.go:226] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0618 10:43:27.374884       1 run.go:72] "command failed" err="finished without leader elect"


==> kubelet <==
Jun 18 11:25:52 minikube kubelet[1773]: E0618 11:25:52.663431    1773 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secretmongodb-configmap,},Key:database_url,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-username,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w2mfk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-express-deployment-6965d87ff4-zdmp6_default(11c73bef-3264-4c0d-a3ad-77ae4739762e): CreateContainerConfigError: configmap \"mongodb-secretmongodb-configmap\" not found" logger="UnhandledError"
Jun 18 11:25:52 minikube kubelet[1773]: E0618 11:25:52.664682    1773 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"mongodb-secretmongodb-configmap\\\" not found\"" pod="default/mongo-express-deployment-6965d87ff4-zdmp6" podUID="11c73bef-3264-4c0d-a3ad-77ae4739762e"
Jun 18 11:25:55 minikube kubelet[1773]: E0618 11:25:55.435323    1773 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:mongodb,Image:mongo,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:27017,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGO_INITDB_ROOT_USERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:krishna-mongodb-secret,},Key:mongo-root-username,Optional:nil,},},},EnvVar{Name:MONGO_INITDB_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pnggh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongodb-deployment-7c4f59c947-q97rs_default(2c0b696b-573a-494e-bd88-1ca0fc625c3a): CreateContainerConfigError: secret \"krishna-mongodb-secret\" not found" logger="UnhandledError"
Jun 18 11:25:55 minikube kubelet[1773]: E0618 11:25:55.436611    1773 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with CreateContainerConfigError: \"secret \\\"krishna-mongodb-secret\\\" not found\"" pod="default/mongodb-deployment-7c4f59c947-q97rs" podUID="2c0b696b-573a-494e-bd88-1ca0fc625c3a"
Jun 18 11:26:02 minikube kubelet[1773]: I0618 11:26:02.311394    1773 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-w2mfk\" (UniqueName: \"kubernetes.io/projected/11c73bef-3264-4c0d-a3ad-77ae4739762e-kube-api-access-w2mfk\") pod \"11c73bef-3264-4c0d-a3ad-77ae4739762e\" (UID: \"11c73bef-3264-4c0d-a3ad-77ae4739762e\") "
Jun 18 11:26:02 minikube kubelet[1773]: I0618 11:26:02.327272    1773 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/11c73bef-3264-4c0d-a3ad-77ae4739762e-kube-api-access-w2mfk" (OuterVolumeSpecName: "kube-api-access-w2mfk") pod "11c73bef-3264-4c0d-a3ad-77ae4739762e" (UID: "11c73bef-3264-4c0d-a3ad-77ae4739762e"). InnerVolumeSpecName "kube-api-access-w2mfk". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Jun 18 11:26:02 minikube kubelet[1773]: I0618 11:26:02.412313    1773 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-w2mfk\" (UniqueName: \"kubernetes.io/projected/11c73bef-3264-4c0d-a3ad-77ae4739762e-kube-api-access-w2mfk\") on node \"minikube\" DevicePath \"\""
Jun 18 11:26:04 minikube kubelet[1773]: I0618 11:26:04.012976    1773 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="11c73bef-3264-4c0d-a3ad-77ae4739762e" path="/var/lib/kubelet/pods/11c73bef-3264-4c0d-a3ad-77ae4739762e/volumes"
Jun 18 11:26:11 minikube kubelet[1773]: E0618 11:26:11.107260    1773 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:mongodb,Image:mongo,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:27017,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGO_INITDB_ROOT_USERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:krishna-mongodb-secret,},Key:mongo-root-username,Optional:nil,},},},EnvVar{Name:MONGO_INITDB_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pnggh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongodb-deployment-7c4f59c947-q97rs_default(2c0b696b-573a-494e-bd88-1ca0fc625c3a): CreateContainerConfigError: secret \"krishna-mongodb-secret\" not found" logger="UnhandledError"
Jun 18 11:26:11 minikube kubelet[1773]: E0618 11:26:11.109211    1773 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with CreateContainerConfigError: \"secret \\\"krishna-mongodb-secret\\\" not found\"" pod="default/mongodb-deployment-7c4f59c947-q97rs" podUID="2c0b696b-573a-494e-bd88-1ca0fc625c3a"
Jun 18 11:26:26 minikube kubelet[1773]: E0618 11:26:26.904509    1773 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:mongodb,Image:mongo,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:27017,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:MONGO_INITDB_ROOT_USERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:krishna-mongodb-secret,},Key:mongo-root-username,Optional:nil,},},},EnvVar{Name:MONGO_INITDB_ROOT_PASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pnggh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongodb-deployment-7c4f59c947-q97rs_default(2c0b696b-573a-494e-bd88-1ca0fc625c3a): CreateContainerConfigError: secret \"krishna-mongodb-secret\" not found" logger="UnhandledError"
Jun 18 11:26:26 minikube kubelet[1773]: E0618 11:26:26.905906    1773 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongodb\" with CreateContainerConfigError: \"secret \\\"krishna-mongodb-secret\\\" not found\"" pod="default/mongodb-deployment-7c4f59c947-q97rs" podUID="2c0b696b-573a-494e-bd88-1ca0fc625c3a"
Jun 18 11:26:29 minikube kubelet[1773]: I0618 11:26:29.357770    1773 reconciler_common.go:162] "operationExecutor.UnmountVolume started for volume \"kube-api-access-pnggh\" (UniqueName: \"kubernetes.io/projected/2c0b696b-573a-494e-bd88-1ca0fc625c3a-kube-api-access-pnggh\") pod \"2c0b696b-573a-494e-bd88-1ca0fc625c3a\" (UID: \"2c0b696b-573a-494e-bd88-1ca0fc625c3a\") "
Jun 18 11:26:29 minikube kubelet[1773]: I0618 11:26:29.365833    1773 operation_generator.go:781] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/2c0b696b-573a-494e-bd88-1ca0fc625c3a-kube-api-access-pnggh" (OuterVolumeSpecName: "kube-api-access-pnggh") pod "2c0b696b-573a-494e-bd88-1ca0fc625c3a" (UID: "2c0b696b-573a-494e-bd88-1ca0fc625c3a"). InnerVolumeSpecName "kube-api-access-pnggh". PluginName "kubernetes.io/projected", VolumeGIDValue ""
Jun 18 11:26:29 minikube kubelet[1773]: I0618 11:26:29.459071    1773 reconciler_common.go:299] "Volume detached for volume \"kube-api-access-pnggh\" (UniqueName: \"kubernetes.io/projected/2c0b696b-573a-494e-bd88-1ca0fc625c3a-kube-api-access-pnggh\") on node \"minikube\" DevicePath \"\""
Jun 18 11:26:30 minikube kubelet[1773]: I0618 11:26:30.010698    1773 kubelet_volumes.go:163] "Cleaned up orphaned pod volumes dir" podUID="2c0b696b-573a-494e-bd88-1ca0fc625c3a" path="/var/lib/kubelet/pods/2c0b696b-573a-494e-bd88-1ca0fc625c3a/volumes"
Jun 18 11:28:55 minikube kubelet[1773]: I0618 11:28:55.468062    1773 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-6pnd7\" (UniqueName: \"kubernetes.io/projected/e7012ee6-2a58-4fbe-8565-e17f7d541c08-kube-api-access-6pnd7\") pod \"mongodb-deployment-6d9d7c68f6-cr8vm\" (UID: \"e7012ee6-2a58-4fbe-8565-e17f7d541c08\") " pod="default/mongodb-deployment-6d9d7c68f6-cr8vm"
Jun 18 11:28:56 minikube kubelet[1773]: I0618 11:28:56.035756    1773 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="af4b7796a6219a39531a36c44f74bee25b9f579aebaddd237739293dc80fad9c"
Jun 18 11:29:00 minikube kubelet[1773]: I0618 11:29:00.146305    1773 pod_startup_latency_tracker.go:104] "Observed pod startup duration" pod="default/mongodb-deployment-6d9d7c68f6-cr8vm" podStartSLOduration=2.298107147 podStartE2EDuration="5.143584406s" podCreationTimestamp="2025-06-18 11:28:55 +0000 UTC" firstStartedPulling="2025-06-18 11:28:56.142631686 +0000 UTC m=+968.944661733" lastFinishedPulling="2025-06-18 11:28:58.988108941 +0000 UTC m=+971.790138992" observedRunningTime="2025-06-18 11:29:00.143374262 +0000 UTC m=+972.945404319" watchObservedRunningTime="2025-06-18 11:29:00.143584406 +0000 UTC m=+972.945614460"
Jun 18 11:29:22 minikube kubelet[1773]: I0618 11:29:22.477517    1773 reconciler_common.go:251] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-z6p4m\" (UniqueName: \"kubernetes.io/projected/ec93887d-fd5f-44c2-898d-9c76daa925b6-kube-api-access-z6p4m\") pod \"mongo-express-deployment-6965d87ff4-9sfml\" (UID: \"ec93887d-fd5f-44c2-898d-9c76daa925b6\") " pod="default/mongo-express-deployment-6965d87ff4-9sfml"
Jun 18 11:29:27 minikube kubelet[1773]: E0618 11:29:27.571618    1773 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secretmongodb-configmap,},Key:database_url,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-username,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z6p4m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-express-deployment-6965d87ff4-9sfml_default(ec93887d-fd5f-44c2-898d-9c76daa925b6): CreateContainerConfigError: configmap \"mongodb-secretmongodb-configmap\" not found" logger="UnhandledError"
Jun 18 11:29:27 minikube kubelet[1773]: E0618 11:29:27.574140    1773 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"mongodb-secretmongodb-configmap\\\" not found\"" pod="default/mongo-express-deployment-6965d87ff4-9sfml" podUID="ec93887d-fd5f-44c2-898d-9c76daa925b6"
Jun 18 11:29:31 minikube kubelet[1773]: E0618 11:29:31.189622    1773 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secretmongodb-configmap,},Key:database_url,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-username,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z6p4m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-express-deployment-6965d87ff4-9sfml_default(ec93887d-fd5f-44c2-898d-9c76daa925b6): CreateContainerConfigError: configmap \"mongodb-secretmongodb-configmap\" not found" logger="UnhandledError"
Jun 18 11:29:31 minikube kubelet[1773]: E0618 11:29:31.190818    1773 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"mongodb-secretmongodb-configmap\\\" not found\"" pod="default/mongo-express-deployment-6965d87ff4-9sfml" podUID="ec93887d-fd5f-44c2-898d-9c76daa925b6"
Jun 18 11:29:44 minikube kubelet[1773]: E0618 11:29:44.783332    1773 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secretmongodb-configmap,},Key:database_url,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-username,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z6p4m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-express-deployment-6965d87ff4-9sfml_default(ec93887d-fd5f-44c2-898d-9c76daa925b6): CreateContainerConfigError: configmap \"mongodb-secretmongodb-configmap\" not found" logger="UnhandledError"
Jun 18 11:29:44 minikube kubelet[1773]: E0618 11:29:44.784711    1773 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"mongodb-secretmongodb-configmap\\\" not found\"" pod="default/mongo-express-deployment-6965d87ff4-9sfml" podUID="ec93887d-fd5f-44c2-898d-9c76daa925b6"
Jun 18 11:30:02 minikube kubelet[1773]: E0618 11:30:02.835335    1773 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secretmongodb-configmap,},Key:database_url,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-username,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z6p4m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-express-deployment-6965d87ff4-9sfml_default(ec93887d-fd5f-44c2-898d-9c76daa925b6): CreateContainerConfigError: configmap \"mongodb-secretmongodb-configmap\" not found" logger="UnhandledError"
Jun 18 11:30:02 minikube kubelet[1773]: E0618 11:30:02.836509    1773 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"mongodb-secretmongodb-configmap\\\" not found\"" pod="default/mongo-express-deployment-6965d87ff4-9sfml" podUID="ec93887d-fd5f-44c2-898d-9c76daa925b6"
Jun 18 11:30:18 minikube kubelet[1773]: E0618 11:30:18.895303    1773 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secretmongodb-configmap,},Key:database_url,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-username,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z6p4m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-express-deployment-6965d87ff4-9sfml_default(ec93887d-fd5f-44c2-898d-9c76daa925b6): CreateContainerConfigError: configmap \"mongodb-secretmongodb-configmap\" not found" logger="UnhandledError"
Jun 18 11:30:18 minikube kubelet[1773]: E0618 11:30:18.897003    1773 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"mongodb-secretmongodb-configmap\\\" not found\"" pod="default/mongo-express-deployment-6965d87ff4-9sfml" podUID="ec93887d-fd5f-44c2-898d-9c76daa925b6"
Jun 18 11:30:33 minikube kubelet[1773]: E0618 11:30:33.750323    1773 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secretmongodb-configmap,},Key:database_url,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-username,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z6p4m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-express-deployment-6965d87ff4-9sfml_default(ec93887d-fd5f-44c2-898d-9c76daa925b6): CreateContainerConfigError: configmap \"mongodb-secretmongodb-configmap\" not found" logger="UnhandledError"
Jun 18 11:30:33 minikube kubelet[1773]: E0618 11:30:33.751565    1773 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"mongodb-secretmongodb-configmap\\\" not found\"" pod="default/mongo-express-deployment-6965d87ff4-9sfml" podUID="ec93887d-fd5f-44c2-898d-9c76daa925b6"
Jun 18 11:30:51 minikube kubelet[1773]: E0618 11:30:51.876561    1773 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secretmongodb-configmap,},Key:database_url,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-username,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z6p4m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-express-deployment-6965d87ff4-9sfml_default(ec93887d-fd5f-44c2-898d-9c76daa925b6): CreateContainerConfigError: configmap \"mongodb-secretmongodb-configmap\" not found" logger="UnhandledError"
Jun 18 11:30:51 minikube kubelet[1773]: E0618 11:30:51.877860    1773 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"mongodb-secretmongodb-configmap\\\" not found\"" pod="default/mongo-express-deployment-6965d87ff4-9sfml" podUID="ec93887d-fd5f-44c2-898d-9c76daa925b6"
Jun 18 11:31:06 minikube kubelet[1773]: E0618 11:31:06.850602    1773 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secretmongodb-configmap,},Key:database_url,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-username,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z6p4m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-express-deployment-6965d87ff4-9sfml_default(ec93887d-fd5f-44c2-898d-9c76daa925b6): CreateContainerConfigError: configmap \"mongodb-secretmongodb-configmap\" not found" logger="UnhandledError"
Jun 18 11:31:06 minikube kubelet[1773]: E0618 11:31:06.851777    1773 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"mongodb-secretmongodb-configmap\\\" not found\"" pod="default/mongo-express-deployment-6965d87ff4-9sfml" podUID="ec93887d-fd5f-44c2-898d-9c76daa925b6"
Jun 18 11:31:22 minikube kubelet[1773]: E0618 11:31:22.897502    1773 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secretmongodb-configmap,},Key:database_url,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-username,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z6p4m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-express-deployment-6965d87ff4-9sfml_default(ec93887d-fd5f-44c2-898d-9c76daa925b6): CreateContainerConfigError: configmap \"mongodb-secretmongodb-configmap\" not found" logger="UnhandledError"
Jun 18 11:31:22 minikube kubelet[1773]: E0618 11:31:22.898707    1773 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"mongodb-secretmongodb-configmap\\\" not found\"" pod="default/mongo-express-deployment-6965d87ff4-9sfml" podUID="ec93887d-fd5f-44c2-898d-9c76daa925b6"
Jun 18 11:31:39 minikube kubelet[1773]: E0618 11:31:39.798975    1773 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secretmongodb-configmap,},Key:database_url,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-username,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z6p4m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-express-deployment-6965d87ff4-9sfml_default(ec93887d-fd5f-44c2-898d-9c76daa925b6): CreateContainerConfigError: configmap \"mongodb-secretmongodb-configmap\" not found" logger="UnhandledError"
Jun 18 11:31:39 minikube kubelet[1773]: E0618 11:31:39.800169    1773 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"mongodb-secretmongodb-configmap\\\" not found\"" pod="default/mongo-express-deployment-6965d87ff4-9sfml" podUID="ec93887d-fd5f-44c2-898d-9c76daa925b6"
Jun 18 11:31:55 minikube kubelet[1773]: E0618 11:31:55.882494    1773 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secretmongodb-configmap,},Key:database_url,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-username,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z6p4m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-express-deployment-6965d87ff4-9sfml_default(ec93887d-fd5f-44c2-898d-9c76daa925b6): CreateContainerConfigError: configmap \"mongodb-secretmongodb-configmap\" not found" logger="UnhandledError"
Jun 18 11:31:55 minikube kubelet[1773]: E0618 11:31:55.883707    1773 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"mongodb-secretmongodb-configmap\\\" not found\"" pod="default/mongo-express-deployment-6965d87ff4-9sfml" podUID="ec93887d-fd5f-44c2-898d-9c76daa925b6"
Jun 18 11:32:13 minikube kubelet[1773]: E0618 11:32:13.897057    1773 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secretmongodb-configmap,},Key:database_url,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-username,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z6p4m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-express-deployment-6965d87ff4-9sfml_default(ec93887d-fd5f-44c2-898d-9c76daa925b6): CreateContainerConfigError: configmap \"mongodb-secretmongodb-configmap\" not found" logger="UnhandledError"
Jun 18 11:32:13 minikube kubelet[1773]: E0618 11:32:13.898299    1773 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"mongodb-secretmongodb-configmap\\\" not found\"" pod="default/mongo-express-deployment-6965d87ff4-9sfml" podUID="ec93887d-fd5f-44c2-898d-9c76daa925b6"
Jun 18 11:32:27 minikube kubelet[1773]: E0618 11:32:27.729691    1773 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secretmongodb-configmap,},Key:database_url,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-username,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z6p4m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-express-deployment-6965d87ff4-9sfml_default(ec93887d-fd5f-44c2-898d-9c76daa925b6): CreateContainerConfigError: configmap \"mongodb-secretmongodb-configmap\" not found" logger="UnhandledError"
Jun 18 11:32:27 minikube kubelet[1773]: E0618 11:32:27.730953    1773 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"mongodb-secretmongodb-configmap\\\" not found\"" pod="default/mongo-express-deployment-6965d87ff4-9sfml" podUID="ec93887d-fd5f-44c2-898d-9c76daa925b6"
Jun 18 11:32:45 minikube kubelet[1773]: E0618 11:32:45.841562    1773 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secretmongodb-configmap,},Key:database_url,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-username,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z6p4m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-express-deployment-6965d87ff4-9sfml_default(ec93887d-fd5f-44c2-898d-9c76daa925b6): CreateContainerConfigError: configmap \"mongodb-secretmongodb-configmap\" not found" logger="UnhandledError"
Jun 18 11:32:45 minikube kubelet[1773]: E0618 11:32:45.842843    1773 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"mongodb-secretmongodb-configmap\\\" not found\"" pod="default/mongo-express-deployment-6965d87ff4-9sfml" podUID="ec93887d-fd5f-44c2-898d-9c76daa925b6"
Jun 18 11:33:00 minikube kubelet[1773]: E0618 11:33:00.793590    1773 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secretmongodb-configmap,},Key:database_url,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-username,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z6p4m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-express-deployment-6965d87ff4-9sfml_default(ec93887d-fd5f-44c2-898d-9c76daa925b6): CreateContainerConfigError: configmap \"mongodb-secretmongodb-configmap\" not found" logger="UnhandledError"
Jun 18 11:33:00 minikube kubelet[1773]: E0618 11:33:00.794791    1773 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"mongodb-secretmongodb-configmap\\\" not found\"" pod="default/mongo-express-deployment-6965d87ff4-9sfml" podUID="ec93887d-fd5f-44c2-898d-9c76daa925b6"
Jun 18 11:33:19 minikube kubelet[1773]: E0618 11:33:19.146200    1773 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secretmongodb-configmap,},Key:database_url,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-username,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z6p4m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-express-deployment-6965d87ff4-9sfml_default(ec93887d-fd5f-44c2-898d-9c76daa925b6): CreateContainerConfigError: configmap \"mongodb-secretmongodb-configmap\" not found" logger="UnhandledError"
Jun 18 11:33:19 minikube kubelet[1773]: E0618 11:33:19.147370    1773 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"mongodb-secretmongodb-configmap\\\" not found\"" pod="default/mongo-express-deployment-6965d87ff4-9sfml" podUID="ec93887d-fd5f-44c2-898d-9c76daa925b6"
Jun 18 11:33:34 minikube kubelet[1773]: E0618 11:33:34.989158    1773 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secretmongodb-configmap,},Key:database_url,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-username,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z6p4m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-express-deployment-6965d87ff4-9sfml_default(ec93887d-fd5f-44c2-898d-9c76daa925b6): CreateContainerConfigError: configmap \"mongodb-secretmongodb-configmap\" not found" logger="UnhandledError"
Jun 18 11:33:34 minikube kubelet[1773]: E0618 11:33:34.990368    1773 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"mongodb-secretmongodb-configmap\\\" not found\"" pod="default/mongo-express-deployment-6965d87ff4-9sfml" podUID="ec93887d-fd5f-44c2-898d-9c76daa925b6"
Jun 18 11:33:52 minikube kubelet[1773]: E0618 11:33:52.417507    1773 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secretmongodb-configmap,},Key:database_url,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-username,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z6p4m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-express-deployment-6965d87ff4-9sfml_default(ec93887d-fd5f-44c2-898d-9c76daa925b6): CreateContainerConfigError: configmap \"mongodb-secretmongodb-configmap\" not found" logger="UnhandledError"
Jun 18 11:33:52 minikube kubelet[1773]: E0618 11:33:52.419033    1773 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"mongodb-secretmongodb-configmap\\\" not found\"" pod="default/mongo-express-deployment-6965d87ff4-9sfml" podUID="ec93887d-fd5f-44c2-898d-9c76daa925b6"
Jun 18 11:34:06 minikube kubelet[1773]: E0618 11:34:06.950737    1773 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secretmongodb-configmap,},Key:database_url,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-username,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z6p4m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-express-deployment-6965d87ff4-9sfml_default(ec93887d-fd5f-44c2-898d-9c76daa925b6): CreateContainerConfigError: configmap \"mongodb-secretmongodb-configmap\" not found" logger="UnhandledError"
Jun 18 11:34:06 minikube kubelet[1773]: E0618 11:34:06.951909    1773 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"mongodb-secretmongodb-configmap\\\" not found\"" pod="default/mongo-express-deployment-6965d87ff4-9sfml" podUID="ec93887d-fd5f-44c2-898d-9c76daa925b6"
Jun 18 11:34:21 minikube kubelet[1773]: E0618 11:34:21.460118    1773 kuberuntime_manager.go:1358] "Unhandled Error" err="container &Container{Name:mongo-express,Image:mongo-express,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8081,Protocol:TCP,HostIP:,},},Env:[]EnvVar{EnvVar{Name:ME_CONFIG_MONGODB_SERVER,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:&ConfigMapKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secretmongodb-configmap,},Key:database_url,Optional:nil,},SecretKeyRef:nil,},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINUSERNAME,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-username,Optional:nil,},},},EnvVar{Name:ME_CONFIG_MONGODB_ADMINPASSWORD,Value:,ValueFrom:&EnvVarSource{FieldRef:nil,ResourceFieldRef:nil,ConfigMapKeyRef:nil,SecretKeyRef:&SecretKeySelector{LocalObjectReference:LocalObjectReference{Name:mongodb-secret,},Key:mongo-root-password,Optional:nil,},},},},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z6p4m,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,RecursiveReadOnly:nil,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:Always,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},RestartPolicy:nil,} start failed in pod mongo-express-deployment-6965d87ff4-9sfml_default(ec93887d-fd5f-44c2-898d-9c76daa925b6): CreateContainerConfigError: configmap \"mongodb-secretmongodb-configmap\" not found" logger="UnhandledError"
Jun 18 11:34:21 minikube kubelet[1773]: E0618 11:34:21.461380    1773 pod_workers.go:1301] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"mongo-express\" with CreateContainerConfigError: \"configmap \\\"mongodb-secretmongodb-configmap\\\" not found\"" pod="default/mongo-express-deployment-6965d87ff4-9sfml" podUID="ec93887d-fd5f-44c2-898d-9c76daa925b6"


==> storage-provisioner [84e3b335ffe2] <==
I0618 11:13:14.739483       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0618 11:13:15.448500       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": x509: certificate signed by unknown authority


==> storage-provisioner [d00e68b432f1] <==
W0618 11:33:34.794222       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:33:34.802222       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:33:36.807686       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:33:36.830982       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:33:38.834311       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:33:38.840652       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:33:40.846261       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:33:40.875804       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:33:42.880432       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:33:42.900584       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:33:44.907026       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:33:44.942914       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:33:46.947457       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:33:46.982201       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:33:48.983994       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:33:49.017783       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:33:51.023721       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:33:51.032704       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:33:53.036161       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:33:53.048366       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:33:55.054544       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:33:55.064784       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:33:57.070693       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:33:57.081609       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:33:59.086907       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:33:59.104780       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:34:01.109235       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:34:01.136254       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:34:03.138853       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:34:03.148676       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:34:05.153705       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:34:05.173104       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:34:07.177191       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:34:07.203010       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:34:09.210442       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:34:09.224746       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:34:11.228705       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:34:11.248657       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:34:13.253056       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:34:13.260399       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:34:15.264955       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:34:15.279558       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:34:17.283292       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:34:17.292100       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:34:19.295491       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:34:19.308288       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:34:21.313894       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:34:21.329055       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:34:23.335629       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:34:23.347321       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:34:25.350098       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:34:25.355877       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:34:27.362142       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:34:27.376718       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:34:29.382631       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:34:29.391311       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:34:31.398716       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:34:31.408301       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:34:33.412729       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0618 11:34:33.422344       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice

